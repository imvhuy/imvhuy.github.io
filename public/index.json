[
{
	"uri": "//localhost:1313/",
	"title": "Building a Serverless Weather ETL Pipeline",
	"tags": [],
	"description": "",
	"content": "Building an ETL Data Pipeline for Weather Analysis on AWS Workshop Overview In this workshop, you will create a simple yet complete weather data pipeline, demonstrating core ETL concepts using AWS serverless technology.\nThis workshop shows how to build a simple ETL pipeline using AWS serverless technology:\nCollect weather data from the OpenWeatherMap API using AWS Lambda Process and transform raw data into an analytics-ready format Store data in Amazon S3 for both raw and processed data Analyze data using Amazon Athena with SQL queries Visualize insights through an Amazon QuickSight dashboard Clean up resources to optimize costs Technologies Used: This workshop uses AWS Lambda, S3, Athena, and QuickSight combined with the OpenWeatherMap API to build a serverless ETL pipeline for collecting and analyzing weather data. Main Sections 1. Introduction Workshop overview and learning objectives Architectural design and introduction to AWS services Prerequisites and setup 2. Data Collection with OpenWeatherMap Set up an OpenWeatherMap API account Create a Lambda function for data collection Configure automated data fetching Test and monitor the collection process 3. Serverless Data Processing with Lambda Build a Lambda function for data transformation Convert raw weather JSON into an analytics format Implement data validation and enrichment Set up processing triggers 4. Data Analysis with Amazon Athena Create an S3 data lake structure Set up Athena tables and schemas Write SQL queries for weather analysis Explore patterns and insights from the data 5. Data Visualization with QuickSight Set up Amazon QuickSight Create a weather dashboard Build interactive visualizations Share and publish the dashboard 6. Resource Cleanup and Next Steps Comprehensive cleanup checklist Cost optimization strategies Suggestions for improvements and extensions Additional learning resources "
},
{
	"uri": "//localhost:1313/2-data-collection-openweathermap/2.1-openweathermap-setup/",
	"title": "2.1 OpenWeatherMap API Setup",
	"tags": [],
	"description": "",
	"content": "In this simplified section, we will quickly set up an OpenWeatherMap API account to collect weather data for our pipeline.\nStep 1: Sign up for OpenWeatherMap Create an account Go to https://openweathermap.org and click \u0026ldquo;Sign Up\u0026rdquo; Complete the registration with your email Verify your email and log in Step 2: Get API Key Access API Keys After logging in, go to the \u0026ldquo;API keys\u0026rdquo; section Note down the default API key or create a new one named \u0026ldquo;weather-data-collection\u0026rdquo; The free plan includes 1,000 API calls per day and 60 calls per minute.\nStep 3: Test API Key Test your API key using one of the following methods:\nBrowser method:\nhttps://api.openweathermap.org/data/2.5/weather?lat=10.7769\u0026amp;lon=106.7009\u0026amp;appid=YOUR_API_KEY cURL method:\ncurl \u0026#34;https://api.openweathermap.org/data/2.5/weather?lat=10.7769\u0026amp;lon=106.7009\u0026amp;appid=YOUR_API_KEY\u0026#34; You should see a JSON response with the current weather data for Ho Chi Minh City.\nMain API Endpoints to be Used # Current weather\rhttps://api.openweathermap.org/data/2.5/weather?q={city}\u0026amp;appid={API key} Next Steps That\u0026rsquo;s it! You now have a working OpenWeatherMap API key securely stored in Parameter Store. In the next section, we will create a Lambda function to collect weather data.\nCompleted:\nCreated OpenWeatherMap account Retrieved API key Stored API key in AWS Parameter Store "
},
{
	"uri": "//localhost:1313/2-data-collection-openweathermap/2.2-lambda-weather-collector/",
	"title": "2.2 Building the Lambda Weather Collector",
	"tags": [],
	"description": "",
	"content": "In this section, we will create AWS Lambda functions to automatically collect weather data from the OpenWeatherMap API and store it in S3. These functions will be the core of our weather data collection system.\nStep 1: Create IAM Role for Lambda 1.1 Create Lambda Execution Role Navigate to IAM Console\nAWS Console → IAM → Roles Click \u0026ldquo;Create role\u0026rdquo; Select Trusted Entity\nService: Lambda Click \u0026ldquo;Next\u0026rdquo; Configure Role\nRole Name: WeatherCollectorLambdaRole Description: Execution role for weather data collection Lambda functions 1.2 Attach AWS Managed Policies What is an AWS Managed Policy?\nAWS has pre-created many common policies that we can attach to a Role instead of writing them from scratch. This saves time and ensures security.\nHow to attach policies in the AWS Console:\nWhen creating the WeatherCollectorLambdaRole, at the \u0026ldquo;Add permissions\u0026rdquo; step \u0026ldquo;Permissions\u0026rdquo; tab → Click \u0026ldquo;Attach policies directly\u0026rdquo; Search for and select each of the following policies: Type AWSLambdaBasicExecutionRole → check the box Type AmazonS3FullAccess → check the box Type AmazonSSMReadOnlyAccess → check the box Type CloudWatchAgentServerPolicy → check the box Click \u0026ldquo;Add Permissions\u0026rdquo; and finish creating the role Result: The WeatherCollectorLambdaRole will have 4 managed policies.\nThis policy allows Lambda to:\nCreate a CloudWatch Log Group to write logs Write logs to CloudWatch when the function runs Basic networking for Lambda to operate Step 2: Create S3 Bucket for Weather Data 2.1 Create Weather Data Bucket Navigate to S3 Console\nAWS Console → S3 → Create bucket Configure Bucket\nBucket Name: weather-data-{your-account-id} (replace with your AWS account ID) Region: us-east-1 (or your preferred region) Block Public Access: Keep all settings enabled (recommended) Bucket Structure weather-data-123456789012/\r├── raw/\r├── current-weather/\r│ └── year=2025/month=01/day=03/hour=10/\r└── year=2025/month=01/day=03/ Step 3: Lambda Function for Current Weather 3.1 Create Current Weather Function Navigate to Lambda Console\nAWS Console → Lambda → Create function Configure Function\nFunction Name: weather-current-collector Runtime: Python 3.11 Architecture: x86_64 Execution Role: Use existing role → WeatherCollectorLambdaRole 3.2 Add Function Code Important: Before copying the code, you need to change the API key. In the line OPENWEATHER_API_KEY = 'API Key\n→ Replace it with your API key from OpenWeatherMap\nIn the Lambda Console → Scroll down to Code source Delete all default code in the lambda_function.py file Copy and paste the following code: File: lambda_function.py\nimport json import boto3 import urllib.request import urllib.parse import urllib.error import os from datetime import datetime, timezone from typing import Dict, List, Optional import logging # Logging configuration logger = logging.getLogger() logger.setLevel(logging.INFO) # AWS clients with retry configuration s3_client = boto3.client(\u0026#39;s3\u0026#39;, config=boto3.session.Config( retries={\u0026#39;max_attempts\u0026#39;: 3, \u0026#39;mode\u0026#39;: \u0026#39;adaptive\u0026#39;} )) cloudwatch = boto3.client(\u0026#39;cloudwatch\u0026#39;, config=boto3.session.Config( retries={\u0026#39;max_attempts\u0026#39;: 3, \u0026#39;mode\u0026#39;: \u0026#39;adaptive\u0026#39;} )) # Configuration BUCKET_NAME = os.environ.get(\u0026#39;WEATHER_BUCKET_NAME\u0026#39;) OPENWEATHER_API_KEY = \u0026#39;your_api_key\u0026#39; # Replace with your actual API key # Target cities for weather data collection CITIES = [ {\u0026#34;name\u0026#34;: \u0026#34;HoChiMinh\u0026#34;, \u0026#34;lat\u0026#34;: 10.7769, \u0026#34;lon\u0026#34;: 106.7009}, {\u0026#34;name\u0026#34;: \u0026#34;Hanoi\u0026#34;, \u0026#34;lat\u0026#34;: 21.0285, \u0026#34;lon\u0026#34;: 105.8542}, {\u0026#34;name\u0026#34;: \u0026#34;Danang\u0026#34;, \u0026#34;lat\u0026#34;: 16.0471, \u0026#34;lon\u0026#34;: 108.2068}, {\u0026#34;name\u0026#34;: \u0026#34;GiaLai\u0026#34;, \u0026#34;lat\u0026#34;: 13.9833, \u0026#34;lon\u0026#34;: 108.0000}, {\u0026#34;name\u0026#34;: \u0026#34;CanTho\u0026#34;, \u0026#34;lat\u0026#34;: 10.0452, \u0026#34;lon\u0026#34;: 105.7469}, {\u0026#34;name\u0026#34;: \u0026#34;Hue\u0026#34;, \u0026#34;lat\u0026#34;: 16.4637, \u0026#34;lon\u0026#34;: 107.5909} ] def fetch_current_weather(city: Dict, api_key: str) -\u0026gt; Optional[Dict]: \u0026#34;\u0026#34;\u0026#34;Fetch current weather data for a city.\u0026#34;\u0026#34;\u0026#34; base_url = \u0026#34;https://api.openweathermap.org/data/2.5/weather\u0026#34; params = { \u0026#39;lat\u0026#39;: str(city[\u0026#39;lat\u0026#39;]), \u0026#39;lon\u0026#39;: str(city[\u0026#39;lon\u0026#39;]), \u0026#39;appid\u0026#39;: api_key, \u0026#39;units\u0026#39;: \u0026#39;metric\u0026#39;, \u0026#39;lang\u0026#39;: \u0026#39;en\u0026#39; } try: # Build URL with parameters query_string = urllib.parse.urlencode(params) full_url = f\u0026#34;{base_url}?{query_string}\u0026#34; # Make HTTP request with urllib.request.urlopen(full_url, timeout=30) as response: if response.status != 200: logger.error(f\u0026#34;HTTP error {response.status} for {city[\u0026#39;name\u0026#39;]}\u0026#34;) return None response_data = response.read().decode(\u0026#39;utf-8\u0026#39;) data = json.loads(response_data) # Add metadata data[\u0026#39;collection_timestamp\u0026#39;] = datetime.now(timezone.utc).isoformat() data[\u0026#39;city_metadata\u0026#39;] = city return data except urllib.error.URLError as e: logger.error(f\u0026#34;URL error for {city[\u0026#39;name\u0026#39;]}: {e}\u0026#34;) return None except urllib.error.HTTPError as e: logger.error(f\u0026#34;HTTP error for {city[\u0026#39;name\u0026#39;]}: {e}\u0026#34;) return None except json.JSONDecodeError as e: logger.error(f\u0026#34;JSON decode error for {city[\u0026#39;name\u0026#39;]}: {e}\u0026#34;) return None except Exception as e: logger.error(f\u0026#34;Unexpected error for {city[\u0026#39;name\u0026#39;]}: {e}\u0026#34;) return None def save_to_s3(data: Dict, city_name: str) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;Save weather data to S3.\u0026#34;\u0026#34;\u0026#34; try: if not BUCKET_NAME: logger.error(\u0026#34;WEATHER_BUCKET_NAME environment variable not set\u0026#34;) return False now = datetime.now(timezone.utc) # Create S3 key with time partitioning (Hive format for analytics) city_safe = city_name.lower().replace(\u0026#39; \u0026#39;, \u0026#39;_\u0026#39;).replace(\u0026#39;.\u0026#39;, \u0026#39;\u0026#39;).replace(\u0026#39;-\u0026#39;, \u0026#39;_\u0026#39;) key = f\u0026#34;raw/current-weather/year={now.year}/month={now.month:02d}/day={now.day:02d}/hour={now.hour:02d}/{city_safe}_{now.strftime(\u0026#39;%Y%m%d_%H%M%S\u0026#39;)}.json\u0026#34; # Prepare body data with UTF-8 encoding json_data = json.dumps(data, ensure_ascii=False, indent=2) body_bytes = json_data.encode(\u0026#39;utf-8\u0026#39;) # Upload file with optimized S3 parameters s3_client.put_object( Bucket=BUCKET_NAME, Key=key, Body=body_bytes, ContentType=\u0026#39;application/json; charset=utf-8\u0026#39;, ContentEncoding=\u0026#39;utf-8\u0026#39;, Metadata={ \u0026#39;city\u0026#39;: city_name, \u0026#39;collection-time\u0026#39;: now.isoformat(), \u0026#39;data-type\u0026#39;: \u0026#39;current-weather\u0026#39;, \u0026#39;source\u0026#39;: \u0026#39;openweathermap\u0026#39; }, ServerSideEncryption=\u0026#39;AES256\u0026#39;, # Server-side encryption StorageClass=\u0026#39;STANDARD_IA\u0026#39; # Save 40% storage cost compared to STANDARD ) logger.info(f\u0026#34;Saved data for {city_name} at s3://{BUCKET_NAME}/{key}\u0026#34;) return True except Exception as e: logger.error(f\u0026#34;S3 save error for {city_name}: {e}\u0026#34;) return False def send_metrics(metric_name: str, value: float, unit: str = \u0026#39;Count\u0026#39;): \u0026#34;\u0026#34;\u0026#34;Send custom metrics to CloudWatch.\u0026#34;\u0026#34;\u0026#34; try: cloudwatch.put_metric_data( Namespace=\u0026#39;Weather/ETL\u0026#39;, MetricData=[ { \u0026#39;MetricName\u0026#39;: metric_name, \u0026#39;Value\u0026#39;: value, \u0026#39;Unit\u0026#39;: unit, \u0026#39;Timestamp\u0026#39;: datetime.now(timezone.utc) } ] ) except Exception as e: logger.error(f\u0026#34;Metric send error {metric_name}: {e}\u0026#34;) "
},
{
	"uri": "//localhost:1313/2-data-collection-openweathermap/2.3-automated-scheduling/",
	"title": "2.3 Automated Scheduling with EventBridge",
	"tags": [],
	"description": "",
	"content": "In this section, we will set up automated scheduling for weather data collection using Amazon EventBridge (formerly CloudWatch Events). This allows Lambda functions to run on a regular schedule without manual intervention.\nEventBridge is like AWS\u0026rsquo;s \u0026ldquo;smart alarm clock\u0026rdquo;:\nRuns on time: Triggers Lambda functions on a specific schedule Precise: Runs at the exact specified time (e.g., every hour, every day) Automated: No manual intervention needed Cost-effective: Only runs when needed, no resource cost when inactive Scheduling for:\nRegular 24/7 data collection Data is always fresh and updated Fully automated Not dependent on an operator Schedule:\nCurrent Weather: Every hour (24 times/day) - To monitor real-time weather Step 1: Set up EventBridge Rule for Current Weather This step will create a schedule to run the Lambda every hour to collect current weather.\n1.1 Access EventBridge Console Go to AWS Console → Search for \u0026ldquo;EventBridge\u0026rdquo; (or \u0026ldquo;CloudWatch\u0026rdquo; → \u0026ldquo;Events\u0026rdquo;) Click \u0026ldquo;EventBridge\u0026rdquo; → \u0026ldquo;Rules\u0026rdquo; in the left menu Select the appropriate Region (e.g., us-east-1) 1.2 Create Rule for Current Weather Click \u0026ldquo;Create rule\u0026rdquo; Step 1 - Define rule detail: Name: weather-current-hourly Description: Collects current weather data every hour for 6 Vietnamese cities Event bus: default Rule type: Schedule Click \u0026ldquo;Continue to create rule\u0026rdquo; 1.3 Configure Schedule Pattern Schedule pattern: Select \u0026ldquo;A schedule that runs at a regular rate, such as every 10 minutes\u0026rdquo;\nRate expression: Select \u0026ldquo;rate\u0026rdquo; and enter:\n1 hour Schedule Pattern Options:\nRate expression:\nrate(1 hour) = every hour rate(30 minutes) = every 30 minutes rate(1 day) = every day Cron expression (advanced):\ncron(0 * * * ? *) = every hour at minute 0 cron(0 8,12,16,20 * * ? *) = 4 times/day (8h, 12h, 16h, 20h) cron(0 0 * * ? *) = every day at 00:00 Click \u0026ldquo;Next\u0026rdquo; 1.4 Select Target (Lambda Function) Target types: Select \u0026ldquo;AWS service\u0026rdquo;\nSelect a service: Select \u0026ldquo;Lambda function\u0026rdquo;\nFunction: Select weather-current-collector (the function created in the previous step)\nAdditional settings: Configure target input: Select \u0026ldquo;Constant (JSON text)\u0026rdquo; JSON text: { \u0026#34;source\u0026#34;: \u0026#34;eventbridge-schedule\u0026#34;, \u0026#34;detail-type\u0026#34;: \u0026#34;Scheduled Event\u0026#34;, \u0026#34;detail\u0026#34;: { \u0026#34;collection_type\u0026#34;: \u0026#34;current_weather\u0026#34;, \u0026#34;scheduled_time\u0026#34;: \u0026#34;hourly\u0026#34;, \u0026#34;trigger_source\u0026#34;: \u0026#34;eventbridge\u0026#34; } } This JSON input will:\nTell Lambda this is a scheduled event (not a manual test) Help differentiate current weather collection Provide metadata for logging and monitoring Click \u0026ldquo;Next\u0026rdquo; 1.5 Configure tags and Review Tags (Optional):\nKey: Project → Value: WeatherETL Key: Environment → Value: Production Review all settings:\nName: weather-current-hourly Schedule: rate(1 hour) Target: weather-current-collector State: Enabled Click \u0026ldquo;Create rule\u0026rdquo;\nThe rule for current weather has been created successfully.\nThe rule will trigger the weather-current-collector Lambda function every hour to collect current weather data.\nEventBridge Setup Complete!\nThe EventBridge rule has been created and will automatically trigger the Lambda function every hour.\nAutomated Workflow: EventBridge → weather-current-collector → S3 → CloudWatch Metrics\nStep 3: Set up Monitoring with CloudWatch Alarms Why is Monitoring Necessary?\nWhen Lambda functions run automatically 24/7, you need to know immediately if there are problems:\nLambda function fails Function runs too long Low success rate Unusual cost increase 3.1 Create SNS Topic for Email Alerts First, create an SNS Topic to receive email notifications for errors:\nAWS Console → \u0026ldquo;SNS\u0026rdquo; → \u0026ldquo;Topics\u0026rdquo; → \u0026ldquo;Create topic\u0026rdquo; Topic configuration:\nType: Standard Name: weather-etl-alerts Display name: Weather ETL Alerts Create topic\nCreate Subscription:\nProtocol: Email Endpoint: your-email@example.com Confirm subscription via email 3.2 Create CloudWatch Alarm for Lambda Errors AWS Console → \u0026ldquo;CloudWatch\u0026rdquo; → \u0026ldquo;Alarms\u0026rdquo; → \u0026ldquo;Create alarm\u0026rdquo;\nSelect metric:\nNamespace: AWS/Lambda Metric name: Errors Dimensions: FunctionName: weather-current-collector Specify metric and conditions:\nStatistic: Sum Period: 5 minutes Threshold type: Static Condition: Greater/Equal Threshold value: 1 (alert when there is ≥1 error in 5 minutes) Configure actions:\nAlarm state trigger: In alarm SNS topic: weather-etl-alerts Add name and description:\nAlarm name: WeatherCurrentCollector-Errors Description: Alert when Lambda weather-current-collector has errors Create alarm Important: Ensure everything is working correctly before letting it run automatically.\nCost and Performance Optimization 1. Smart Scheduling Strategy Instead of running at the same frequency 24/7, you can optimize:\nPeak hours (6:00-23:00): Every hour Off-peak hours (23:00-6:00): Every 2 hours Weekends: Every 2 hours (fewer people care about work-related weather) Create multiple rules with different schedules:\nRule 1 - Peak Hours:\ncron(0 6-23 * * ? *) Rule 2 - Off-peak Hours:\ncron(0 0,2,4 * * ? *) 2. Regional Optimization If you need to collect data for multiple regions:\n{ "
},
{
	"uri": "//localhost:1313/1-introduction/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "Building an ETL Data Pipeline for Weather Analysis on AWS Objectives Build a serverless data collection system using AWS Lambda Implement a data transformation and processing workflow Store and query data using Amazon S3 and Athena Create visualizations with Amazon QuickSight Apply AWS best practices for cost optimization and resource cleanup Architecture Overview Our weather ETL pipeline follows this simple serverless architecture: Key Components:\nData Source: OpenWeatherMap API for real-time weather data Collection: AWS Lambda function to fetch weather data Storage: Amazon S3 for both raw and processed data Processing: AWS Lambda for data transformation Analysis: Amazon Athena for SQL queries Visualization: Amazon QuickSight for dashboards "
},
{
	"uri": "//localhost:1313/2-data-collection-openweathermap/",
	"title": "Data Collection with OpenWeatherMap",
	"tags": [],
	"description": "",
	"content": "In this section, we will learn how to set up automated weather data collection using the OpenWeatherMap API and AWS Lambda. This is the foundation of our weather analytics ETL pipeline, where we will build a reliable serverless data collection system.\n2.1 OpenWeatherMap Setup API and Credentials Setup\nSet up an OpenWeatherMap account, get an API key, and configure Systems Manager Parameter Store to securely store credentials. You will learn how to manage API keys and test connectivity.\n2.2 Lambda Weather Collector Building Data Collection Functions\nCreate Lambda functions to collect current and forecast weather data from the OpenWeatherMap API. This includes IAM roles, S3 bucket setup, and function code with error handling.\n2.3 Automated Scheduling Automated Scheduling with CloudWatch Events\nSet up CloudWatch Events to run Lambda functions on an automated schedule. Configure monitoring, alarms, and notifications to ensure the system runs smoothly.\n2.4 Testing and Monitoring Comprehensive Testing and Monitoring\nEstablish a testing strategy including manual testing, data quality validation, performance testing, and automated health checks. Create a dashboard to monitor the system.\nArchitecture Overview graph TD\rA[OpenWeatherMap API\u0026lt;br/\u0026gt;Current Weather Data] --\u0026gt; B[Lambda Function\u0026lt;br/\u0026gt;weather-current-collector]\rC[EventBridge Rule\u0026lt;br/\u0026gt;Every Hour] --\u0026gt; B\rB --\u0026gt; D[S3 Raw Storage\u0026lt;br/\u0026gt;current-weather/]\rB --\u0026gt; E[CloudWatch Logs\u0026lt;br/\u0026gt;Monitoring]\rB --\u0026gt; F[CloudWatch Metrics\u0026lt;br/\u0026gt;Success/Error Counts]\rG[6 Vietnamese Cities\u0026lt;br/\u0026gt;HCM, Hanoi, Danang\u0026lt;br/\u0026gt;GiaLai, CanTho, Hue] --\u0026gt; A\rstyle A fill:#e1f5fe,stroke:#01579b,stroke-width:2px\rstyle B fill:#ff9900,stroke:#232f3e,stroke-width:3px\rstyle C fill:#e8f5e8,stroke:#2e7d32,stroke-width:2px\rstyle D fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px\rstyle E fill:#fff3e0,stroke:#ef6c00,stroke-width:2px\rstyle F fill:#e3f2fd,stroke:#1976d2,stroke-width:2px\rstyle G fill:#f1f8e9,stroke:#558b2f,stroke-width:2px Data Types Collected Collect current weather data for 6 Vietnamese cities/provinces: Ha Noi, Ho Chi Minh City, Da NangGia Lai, Can Tho, Hue\nData Collected:\nTemperature (°C, °F), humidity, pressure Wind speed, wind direction, cloud cover Weather description, weather condition Metadata: timestamp, location, collection info Collection Schedule Current Weather: Every hour (24 times/day × 6 cities = 144 data points/day)\nCost Estimation Service Usage Cost OpenWeatherMap API 144 calls/day Free Lambda Executions 720 invocations/month Free Tier S3 Storage 500 MB data Free Tier CloudWatch Logs 2 GB logs $1.00 Total ~$1.00/month OpenWeatherMap provides 1,000 free API calls per day, which is sufficient for this workshop.\nExpected Outcomes After completing this module, you will have:\nA 24/7 operational serverless weather data collection system Structured weather data stored in S3 A complete monitoring and alerting system Knowledge of AWS Lambda, CloudWatch Events, and S3 integration Getting Started Ready to build a weather data collection system? Start with 2.1 OpenWeatherMap Setup to set up your API and credentials.\n"
},
{
	"uri": "//localhost:1313/3-serverless-processing-lambda/",
	"title": "Serverless Data Processing and Transformation",
	"tags": [],
	"description": "",
	"content": "In this module, we will build Lambda functions to process and transform the raw weather data from module 2 into a format suitable for analysis. This is the \u0026ldquo;Transform\u0026rdquo; step in our ETL pipeline, which helps clean, standardize, and create meaningful metrics from raw data.\nRaw data from the OpenWeatherMap API has several issues:\nComplex structure: JSON is hard to query Inconsistent units: Kelvin, m/s, Pascal\u0026hellip; Redundant data: Many unnecessary fields Lacks insights: No derived metrics Therefore, processing is needed to get:\nFlat structure: Easy to query with SQL Consistent units: Celsius, km/h, %\u0026hellip; Processing Flow:\nRaw data from Module 2 is saved to S3 S3 Event triggers the Lambda processor Lambda transforms the data and saves it to the processed bucket Processed data is ready for analytics (Module 4) Step 1: Create S3 Bucket for Processed Data Why separate buckets?\nSeparation of concerns: Raw vs. Processed data Security: Different access permissions Cost optimization: Different storage classes Analytics: Processed data is optimized for queries 1.1 Create Processed Data Bucket AWS Console → S3 → Create bucket\nBucket configuration:\nBucket name: weather-processed-{your-account-id} Region: ap-southeast-1 (same as raw bucket) Block all public access: Enabled Bucket versioning: Disable Default encryption: SSE-S3 Create bucket 1.2 Create Folder Structure Create a folder structure optimized for analytics:\nweather-processed-{account-id}/\r├── current-weather/\r├── year=2025/\r│ ├── month=01/\r│ │ ├── day=03/\r│ │ │ ├── hour=00/\r│ │ │ │ ├── hcm_20250103_000000.json\r│ │ │ │ ├── hanoi_20250103_000000.json\r│ │ │ │ └── ...\r│ │ │ └── hour=01/\r│ │ └── day=04/\r│ └── month=02/\r└── year=2026/ We will use Hive-style partitioning because:\nAthena optimization: Better query performance Cost savings: Only scans necessary data Easy filtering: Filter by year/month/day/hour Scalability: Efficiently handles large datasets Step 2: Create Lambda Function for Data Processing 2.1 Create Lambda Function AWS Console → Lambda → Create function\nFunction configuration:\nFunction name: weather-data-processor Runtime: Python 3.11 Architecture: x86_64 Execution role: WeatherCollectorLambdaRole Advanced settings: Memory: 512 MB Timeout: 5 minutes Environment variables: PROCESSED_BUCKET_NAME: weather-processed-{your-account-id} LOG_LEVEL: INFO 2.2 Lambda Function Code Replace the default code with the following:\nimport json import boto3 import datetime import logging from decimal import Decimal import os from urllib.parse import unquote_plus # Logging configuration logger = logging.getLogger() logger.setLevel(logging.INFO) # AWS clients with retry configuration s3_client = boto3.client(\u0026#39;s3\u0026#39;, config=boto3.session.Config( retries={\u0026#39;max_attempts\u0026#39;: 3, \u0026#39;mode\u0026#39;: \u0026#39;adaptive\u0026#39;} )) def lambda_handler(event, context): \u0026#34;\u0026#34;\u0026#34; Main Lambda handler for weather data processing \u0026#34;\u0026#34;\u0026#34; try: # Environment variable validation processed_bucket = os.environ.get(\u0026#39;PROCESSED_BUCKET_NAME\u0026#39;) if not processed_bucket: logger.error(\u0026#34;PROCESSED_BUCKET_NAME environment variable is required\u0026#34;) return { \u0026#39;statusCode\u0026#39;: 500, \u0026#39;body\u0026#39;: json.dumps({ \u0026#39;error\u0026#39;: \u0026#39;Configuration error\u0026#39;, \u0026#39;message\u0026#39;: \u0026#39;PROCESSED_BUCKET_NAME environment variable not set\u0026#39; }) } processed_count = 0 # Process each S3 event record for record in event[\u0026#39;Records\u0026#39;]: try: # Extract bucket and key from event source_bucket = record[\u0026#39;s3\u0026#39;][\u0026#39;bucket\u0026#39;][\u0026#39;name\u0026#39;] source_key_encoded = record[\u0026#39;s3\u0026#39;][\u0026#39;object\u0026#39;][\u0026#39;key\u0026#39;] source_key = unquote_plus(source_key_encoded) logger.info(f\u0026#34;Processing file: {source_key} from bucket: {source_bucket}\u0026#34;) if source_key_encoded != source_key: logger.debug(f\u0026#34;URL decoded key from {source_key_encoded} to {source_key}\u0026#34;) # Get raw weather data from S3 response = s3_client.get_object(Bucket=source_bucket, Key=source_key) raw_data = json.loads(response[\u0026#39;Body\u0026#39;].read().decode(\u0026#39;utf-8\u0026#39;)) # Transform weather data processed_data = transform_weather_data(raw_data) # Create key for processed file processed_key = source_key.replace(\u0026#39;raw/\u0026#39;, \u0026#39;processed/\u0026#39;).replace(\u0026#39;.json\u0026#39;, \u0026#39;.jsonl\u0026#39;) # Save processed data to S3 as NDJSON (jsonl) if isinstance(processed_data, list): jsonl_content = \u0026#39;\\n\u0026#39;.join(json.dumps(obj, default=decimal_default) for obj in processed_data) else: jsonl_content = json.dumps(processed_data, default=decimal_default) s3_client.put_object( Bucket=processed_bucket, Key=processed_key, Body=jsonl_content, ContentType=\u0026#39;application/jsonl; charset=utf-8\u0026#39;, ServerSideEncryption=\u0026#39;AES256\u0026#39; ) processed_count += 1 logger.info(f\u0026#34;Successfully processed and saved: {processed_key}\u0026#34;) except Exception as e: logger.error(f\u0026#34;Error processing record {source_key}: {str(e)}\u0026#34;) continue return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: json.dumps({ \u0026#39;message\u0026#39;: f\u0026#39;Successfully processed {processed_count} files\u0026#39;, \u0026#39;processedCount\u0026#39;: processed_count }) } except Exception as e: logger.error(f\u0026#34;Lambda execution error: {str(e)}\u0026#34;) return { \u0026#39;statusCode\u0026#39;: 500, \u0026#39;body\u0026#39;: json.dumps({ \u0026#39;error\u0026#39;: str(e) }) } def transform_weather_data(raw_data): \u0026#34;\u0026#34;\u0026#34; Transform raw OpenWeatherMap data into analytics-friendly format \u0026#34;\u0026#34;\u0026#34; try: # Extract timestamp (use collection_timestamp if available, otherwise dt) if \u0026#39;collection_timestamp\u0026#39; in raw_data: timestamp = raw_data[\u0026#39;collection_timestamp\u0026#39;] collection_date = datetime.datetime.fromisoformat(timestamp.replace(\u0026#39;Z\u0026#39;, \u0026#39;+00:00\u0026#39;)).strftime(\u0026#39;%Y-%m-%d\u0026#39;) else: timestamp = datetime.datetime.fromtimestamp(raw_data[\u0026#39;dt\u0026#39;]).isoformat() + \u0026#39;Z\u0026#39; collection_date = datetime.datetime.fromtimestamp(raw_data[\u0026#39;dt\u0026#39;]).strftime(\u0026#39;%Y-%m-%d\u0026#39;) # Extract basic location and weather information processed_data = { \u0026#39;timestamp\u0026#39;: timestamp, \u0026#39;city_name\u0026#39;: raw_data.get(\u0026#39;name\u0026#39;, \u0026#39;Unknown\u0026#39;), \u0026#39;country\u0026#39;: raw_data.get(\u0026#39;sys\u0026#39;, {}).get(\u0026#39;country\u0026#39;, \u0026#39;Unknown\u0026#39;), \u0026#39;latitude\u0026#39;: raw_data.get(\u0026#39;coord\u0026#39;, {}).get(\u0026#39;lat\u0026#39;), \u0026#39;longitude\u0026#39;: raw_data.get(\u0026#39;coord\u0026#39;, {}).get(\u0026#39;lon\u0026#39;), \u0026#39;data_collection_date\u0026#39;: collection_date } # Add custom city metadata if available if \u0026#39;city_metadata\u0026#39; in raw_data: city_meta = raw_data[\u0026#39;city_metadata\u0026#39;] processed_data.update({ \u0026#39;city_name\u0026#39;: city_meta.get(\u0026#39;name\u0026#39;, processed_data[\u0026#39;city_name\u0026#39;]), \u0026#39;latitude\u0026#39;: city_meta.get(\u0026#39;lat\u0026#39;, processed_data[\u0026#39;latitude\u0026#39;]), \u0026#39;longitude\u0026#39;: city_meta.get(\u0026#39;lon\u0026#39;, processed_data[\u0026#39;longitude\u0026#39;]) }) # Temperature conversion (OpenWeatherMap returns Celsius when units=metric) temp_celsius = raw_data.get(\u0026#39;main\u0026#39;, {}).get(\u0026#39;temp\u0026#39;) if temp_celsius: processed_data[\u0026#39;temperature_celsius\u0026#39;] = round(temp_celsius, 2) processed_data[\u0026#39;temperature_fahrenheit\u0026#39;] = round(temp_celsius * 9/5 + 32, 2) processed_data[\u0026#39;temperature_kelvin\u0026#39;] = round(temp_celsius + 273.15, 2) # Feels like temperature feels_like_celsius = raw_data.get(\u0026#39;main\u0026#39;, {}).get(\u0026#39;feels_like\u0026#39;) if feels_like_celsius: processed_data[\u0026#39;feels_like_celsius\u0026#39;] = round(feels_like_celsius, 2) processed_data[\u0026#39;feels_like_fahrenheit\u0026#39;] = round(feels_like_celsius * 9/5 + 32, 2) "
},
{
	"uri": "//localhost:1313/4-data-storage-solutions/",
	"title": "Data Analysis with Athena",
	"tags": [],
	"description": "",
	"content": "In this section, we will use Amazon Athena to run SQL queries directly on the processed weather data stored in S3. Athena is a serverless query service that makes it easy to analyze data using standard SQL without needing to set up a complex data warehouse infrastructure.\nObjectives: Set up Amazon Athena to query data from S3 Create a database and an external table for the weather data Write SQL queries to analyze weather data Use enriched fields like comfort_level, weather_severity, and wind_condition Create reports and insights from weather data Optimize performance and cost for Athena queries Prerequisites Completed Module 3: Data Processing and Transformation Have processed weather data in S3 in the provided JSON format AWS Console access with Administrator or permissions for Athena, S3 Basic understanding of SQL (SELECT, WHERE, GROUP BY, etc.) Step 1: Set up S3 Bucket for Athena Query Results Athena needs an S3 bucket to store query results. We will create this bucket manually through the AWS Console.\n1.1 Create S3 Bucket for Query Results Step 1: Access S3 Console\nLog in to the AWS Console Search for and select the S3 service Click Create bucket Step 2: Configure Bucket\nBucket name: Enter a unique name, e.g., weather-athena-query-results-[your-account-id] Region: Select the same region as your weather data bucket Object Ownership: Leave the default ACLs disabled Block Public Access: Leave the default (block all public access) Step 3: Advanced Settings\nBucket Versioning: Disable (not necessary for query results) Default encryption: Enable with Amazon S3 managed keys (SSE-S3) Click Create bucket 1.2 Set up Lifecycle Policy (Optional) To automatically delete old query results and save costs:\nStep 1: Go to Bucket Management\nClick on the newly created bucket Select the Management tab Click Create lifecycle rule Step 2: Configure Lifecycle Rule\nRule name: delete-old-query-results Status: Enable Rule scope: Apply to all objects in the bucket Lifecycle rule actions: Expire current versions of objects Expire current versions of objects: 30 days Click Create rule This lifecycle policy will automatically delete query results after 30 days to save on storage costs.\n1.3 Configure Athena Query Result Location Step 1: Access Athena Console\nSearch for and select the Amazon Athena service Step 2: Set up Query Result Location\nSelect Query editor from the right-hand menu Click Settings in the top right corner Click Manage Step 3: Configure Location\nQuery result location: Select Browse S3 and choose the results bucket you just created. Remember to add a / at the end of the S3 path!\nExpected bucket owner: Leave blank Encrypt query results: Check the box and select SSE-S3 Click Save Step 2: Create Database and External Table 2.1 Create Weather Analytics Database Step 1: Open Query Editor\nIn the Athena Console, select Query editor from the left menu Ensure you are in Data source: AwsDataCatalog Database: default Step 2: Create Database\nCopy and run the following query in the Athena Query Editor:\nCREATE DATABASE IF NOT EXISTS weather_analytics LOCATION \u0026#39;s3://weather-athena-query-results-[your-account-id]/databases/weather_analytics/\u0026#39;; Step 3: Select Database\nAfter the query runs successfully, refresh the page In the Database dropdown, select weather_analytics 2.2 Create External Table for Weather Data Step 1: Prepare Schema\nBased on the processed JSON structure from before, we will create an external table:\nCREATE EXTERNAL TABLE IF NOT EXISTS weather_analytics.current_weather ( timestamp STRING, city_name STRING, country STRING, latitude DOUBLE, longitude DOUBLE, data_collection_date STRING, temperature_celsius DOUBLE, temperature_fahrenheit DOUBLE, temperature_kelvin DOUBLE, feels_like_celsius DOUBLE, feels_like_fahrenheit DOUBLE, humidity_percent INT, pressure_hpa INT, visibility_meters BIGINT, uv_index DOUBLE, weather_id INT, weather_main STRING, weather_description STRING, weather_icon STRING, wind_speed_ms DOUBLE, wind_direction_deg INT, wind_gust_ms DOUBLE, wind_speed_kmh DOUBLE, wind_speed_mph DOUBLE, cloud_coverage_percent INT, heat_index_fahrenheit DOUBLE, heat_index_celsius DOUBLE, comfort_level STRING, wind_condition STRING, weather_severity STRING ) ROW FORMAT SERDE \u0026#39;org.openx.data.jsonserde.JsonSerDe\u0026#39; WITH SERDEPROPERTIES ( \u0026#39;serialization.format\u0026#39; = \u0026#39;1\u0026#39; ) LOCATION \u0026#39;s3://your-weather-processed-bucket/processed/\u0026#39; TBLPROPERTIES ( \u0026#39;has_encrypted_data\u0026#39;=\u0026#39;false\u0026#39;, \u0026#39;projection.enabled\u0026#39;=\u0026#39;true\u0026#39;, \u0026#39;projection.data_collection_date.type\u0026#39;=\u0026#39;date\u0026#39;, \u0026#39;projection.data_collection_date.range\u0026#39;=\u0026#39;2025-01-01,NOW\u0026#39;, \u0026#39;projection.data_collection_date.format\u0026#39;=\u0026#39;yyyy-MM-dd\u0026#39;, \u0026#39;storage.location.template\u0026#39;=\u0026#39;s3://your-weather-processed-bucket/processed/year=${data_collection_date}\u0026#39; ); Schema Explanation:\nThis schema exactly matches the JSON structure from module 3 Includes new fields: comfort_level, wind_condition, weather_severity Uses JsonSerDe to parse JSON files Has partition projection for better performance 2.3 Verify Table Creation Test Basic Query\nSELECT COUNT(*) as total_records FROM weather_analytics.current_weather; Sample Data\nSELECT * FROM weather_analytics.current_weather ORDER BY timestamp DESC LIMIT 5; Step 3: Basic Data Analysis 3.1 Data Quality Check Check data quality:\nSELECT COUNT(*) as total_records, COUNT(DISTINCT city_name) as unique_cities, COUNT(DISTINCT data_collection_date) as unique_dates, COUNT(temperature_celsius) as temp_records, COUNT(comfort_level) as comfort_records, COUNT(weather_severity) as severity_records, MIN(data_collection_date) as earliest_date, MAX(data_collection_date) as latest_date FROM weather_analytics.current_weather; "
},
{
	"uri": "//localhost:1313/5-analytics-visualization/",
	"title": "Data Visualization with QuickSight",
	"tags": [],
	"description": "",
	"content": "Once the weather data is stored and queryable via Athena, it\u0026rsquo;s time for visualization. In this section, we will use Amazon QuickSight to build an interactive dashboard that brings the weather data to life.\nOverview Amazon QuickSight is AWS\u0026rsquo;s business intelligence (BI) service that makes it easy to create and publish interactive dashboards. You will connect QuickSight to your Athena data source and create visualizations that reveal patterns and insights from the weather data.\nStep 1: Set up Amazon QuickSight 1.1 Sign up for QuickSight Access QuickSight Log in to the AWS Console. In the search bar, type QuickSight and select Amazon QuickSight. Click Sign up for QuickSight Choose Standard Edition (includes a 30-day free trial) Enter your account information: Account name: weather-analytics-[your-name] Notification email: Your email address Click Finish 1.2 Configure QuickSight Permissions In QuickSight, click the profile icon (top right) Select Manage QuickSight Choose Security \u0026amp; permissions Click Manage Enable the following services: Amazon Athena Amazon S3 For S3, click Select S3 buckets Select your weather data buckets: your-weather-processed-bucket your-athena-query-results-bucket Click Save Step 2: Create Data Source and Dataset 2.1 Connect to Athena On the QuickSight homepage, click Datasets Click New dataset Select Athena as the data source Configure the connection: Data source name: Weather-Data-Athena Athena workgroup: primary (default) Click Create data source 2.2 Create Dataset from Weather Table Select database: weather_analytics Select table: current_weather Choose Directly query your data Click Visualize If your dataset is small (\u0026lt; 1GB), you can choose \u0026ldquo;Import to SPICE\u0026rdquo; for better performance. SPICE is QuickSight\u0026rsquo;s in-memory calculation engine.\nStep 3: Create Weather Visualizations 3.1 Temperature Trend Line Chart Create a new analysis:\nClick + Add → Add visual Choose Line chart Configure the chart:\nX-axis: Drag data_collection_date to the X-axis Value: Drag temperature_celsius to Value Color: Drag city_name to Color Customize the chart:\nClick the visual → Format visual Title: \u0026ldquo;Temperature Trends by City\u0026rdquo; Y-axis label: \u0026ldquo;Temperature (°C)\u0026rdquo; Legend: Place at the bottom 3.2 Weather Conditions Pie Chart Add a new visual:\nClick + Add → Add visual Choose Pie chart Configure the chart:\nGroup/Color: Drag weather_main to Group/Color Value: Drag city_name to Value Aggregate: Change to Count Customize:\nTitle: \u0026ldquo;Weather Conditions Distribution\u0026rdquo; Legend: Show percentages 3.3 City Temperature Comparison Bar Chart Add a new visual:\nChoose Vertical bar chart Configure:\nX-axis: Drag city_name to the X-axis Value: Drag temperature_celsius to Value Aggregate: Change to Average Customize:\nTitle: \u0026ldquo;Average Temperature by City\u0026rdquo; Y-axis label: \u0026ldquo;Average Temperature (°C)\u0026rdquo; Sort by value (descending) 3.4 Line chart comparing temperature_celsius and feels_like_celsius Add a new visual:\nChoose Line chart Configure:\nX-axis: Drag timestamp to the X-axis Value: Drag temperature_celsius and feels_like_celsius to Value Aggregate: Change to Average Customize:\nTitle: \u0026ldquo;Comparison of Outdoor Temperature and Feels Like Temperature\u0026rdquo; X-axis label: \u0026ldquo;Time\u0026rdquo; 3.5 Key Performance Indicators (KPIs) Create KPI tiles for the latest weather data:\nCurrent Temperature KPI Add new visual → KPI Configure: Value: temperature_celsius Aggregate: Average Filter: Add a filter for the latest date Title: \u0026ldquo;Current Average Temperature\u0026rdquo; Humidity KPI Add KPI visual Configure: Value: humidity_percent Aggregate: Average Title: \u0026ldquo;Current Average Humidity\u0026rdquo; Step 4: Build a Comprehensive Dashboard 4.1 Organize Dashboard Layout Resize and arrange visuals:\nPlace KPIs at the top in a row Temperature trend chart in the main area Pie chart and bar chart side-by-side below Add dashboard title:\nClick + Add → Add title Text: \u0026ldquo;Weather Analysis Dashboard\u0026rdquo; Style: Large, centered 4.2 Add Interactive Filters Add date filter:\nClick the Filter pane (left) Click Create one → Select data_collection_date Filter type: Date range Default: Last 7 days Add city filter:\nCreate a filter for city_name Filter type: Multi-select dropdown Show all cities by default Add weather condition filter:\nCreate a filter for weather_main Filter type: Multi-select dropdown 4.3 Apply Dashboard Styling Choose a color theme:\nClick Themes (top menu) Choose Midnight or Classic Customize colors:\nFor the temperature chart: Use a blue-red gradient For weather conditions: Use distinct colors for each condition Add descriptive text:\nClick + Add → Add text box Add insights or instructions for dashboard users Step 5: Advanced Visualization Wind Direction Chart (Using Calculated Fields) Create a calculated field:\nClick + Add → Add calculated field Name: wind_direction_category Formula: ifelse(\rwind_direction_deg \u0026gt;= 337.5 OR wind_direction_deg \u0026lt; 22.5, \u0026#34;N\u0026#34;,\rwind_direction_deg \u0026gt;= 22.5 AND wind_direction_deg \u0026lt; 67.5, \u0026#34;NE\u0026#34;,\rwind_direction_deg \u0026gt;= 67.5 AND wind_direction_deg \u0026lt; 112.5, \u0026#34;E\u0026#34;,\rwind_direction_deg \u0026gt;= 112.5 AND wind_direction_deg \u0026lt; 157.5, \u0026#34;SE\u0026#34;,\rwind_direction_deg \u0026gt;= 157.5 AND wind_direction_deg \u0026lt; 202.5, \u0026#34;S\u0026#34;,\rwind_direction_deg \u0026gt;= 202.5 AND wind_direction_deg \u0026lt; 247.5, \u0026#34;SW\u0026#34;,\rwind_direction_deg \u0026gt;= 247.5 AND wind_direction_deg \u0026lt; 292.5, \u0026#34;W\u0026#34;,\r\u0026#34;NW\u0026#34;\r) Create wind direction chart:\n"
},
{
	"uri": "//localhost:1313/6-cleanup-next-steps/",
	"title": "Resource Cleanup",
	"tags": [],
	"description": "",
	"content": "After completing the hands-on workshop, it is crucial to clean up resources to prevent unexpected charges.\n6.1 QuickSight Resources Delete Dashboard We will clean up in the reverse order of creation to avoid dependency errors (e.g., you can\u0026rsquo;t delete an S3 bucket if it\u0026rsquo;s still being used by another service).\nStep 1: Unsubscribe from Amazon QuickSight QuickSight is a service with a monthly subscription fee, so prioritize unsubscribing from it first.\nAccess QuickSight: Open the AWS Console, search for and select QuickSight. Click Go to QuickSight to enter the management interface. Delete Assets (Analyses, Dashboards, Datasets): In the left menu, go to each section: Dashboards, Analyses, and Datasets. For each section, select all assets related to the workshop (e.g., \u0026ldquo;Weather Analysis Dashboard\u0026rdquo;) and click Delete. Confirm the deletion. Unsubscribe: In the top right corner, click your profile icon and select Manage QuickSight. In the left menu, select Your Account. Click Manage account. A confirmation dialog will appear. Confirm and proceed to delete the account. Step 2: Delete Amazon Athena Resources Athena itself doesn\u0026rsquo;t cost anything, but the query results stored in S3 do.\nAccess Athena: Open the AWS Console, search for and select Athena. Drop Table: In the Query editor, make sure you have selected the weather_analytics database. Run the following command to delete the table: DROP TABLE IF EXISTS weather_analytics.current_weather; Drop Database: After deleting the table, run the following command: DROP DATABASE IF EXISTS weather_analytics; Step 3: Delete Amazon S3 Buckets S3 charges for storage, so deleting buckets is very important.\nYou must empty the bucket (delete all objects) before you can delete the bucket.\nAccess S3: Open the AWS Console, search for and select S3. Empty and Delete Each Bucket: Perform the following steps for all 3 buckets: your-weather-raw-bucket-[ID] your-weather-processed-bucket-[ID] your-athena-query-results-bucket-[ID] Steps for each bucket: Click the bucket name to open it. Select all objects and folders inside. Click the Delete button. In the confirmation screen, type permanently delete and click Delete objects. After the bucket is empty, go back to the list of buckets. Select the (empty) bucket and click Delete. In the confirmation screen, type the bucket name and click Delete bucket. Step 4: Delete Scheduling and Lambda Resources Delete EventBridge (CloudWatch Events) Rule: Open the AWS Console, search for and select Amazon EventBridge. In the left menu, select Rules. Select the rule you created (e.g., weather-collection-schedule). Click Delete and confirm. Delete Lambda Functions: Open the AWS Console, search for and select Lambda. Delete each function one by one: Select the weather-data-collector function. Click Actions \u0026gt; Delete. Confirm deletion. Select the weather-data-processor function. Click Actions \u0026gt; Delete. Confirm deletion. Step 5: Delete CloudWatch Log Groups Lambda automatically creates log groups. They take up space and can incur small charges.\nAccess CloudWatch: Open the AWS Console, search for and select CloudWatch. Delete Log Groups: In the left menu, select Log groups. Find and select the following log groups: /aws/lambda/weather-data-collector /aws/lambda/weather-data-processor Click Actions \u0026gt; Delete log group(s) and confirm. Step 6: Delete IAM Roles and Policies This is an important step to clean up unnecessary permissions.\nAccess IAM: Open the AWS Console, search for and select IAM. Delete IAM Role for Lambda: In the left menu, select Roles. Find the role you created for Lambda (e.g., weather-lambda-role). Note: If this role has attached policies, you need to detach them before you can delete the role. Usually, when you delete a role, AWS automatically detaches the policies you created. Select the role, click Delete and confirm. Delete IAM Role for QuickSight (if you created one separately): Repeat the process for the QuickSight role (e.g., quicksight-athena-role). Final Verification After completing the above steps, double-check everything:\nCheck Billing Dashboard: Go to AWS Billing and see if there are any unusual charges in the next few hours. Check Services: Glance through the S3, Lambda, and IAM consoles to ensure no resources related to the workshop remain. "
},
{
	"uri": "//localhost:1313/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "//localhost:1313/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]