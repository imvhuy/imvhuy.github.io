[
{
	"uri": "//localhost:1313/",
	"title": "Building ETL Data Pipeline for Weather Analytics on AWS",
	"tags": [],
	"description": "",
	"content": "Building ETL Data Pipeline for Weather Analytics on AWS Overview In this comprehensive workshop, you will learn how to build a serverless ETL (Extract, Transform, Load) data pipeline for weather analytics using AWS services and real-time weather data from OpenWeatherMap API. This hands-on workshop will guide you through creating a simple, cost-effective data processing system that collects, transforms, and analyzes real weather data including current conditions, forecasts, and historical trends across multiple cities.\nWhat You\u0026rsquo;ll Build You will create a complete serverless data pipeline that:\nCollects real-time weather data from OpenWeatherMap API Processes and transforms weather data using AWS Lambda Stores structured weather data in S3 Data Lake Analyzes weather patterns using Amazon Athena Visualizes weather insights through QuickSight dashboards This workshop is designed for developers, data engineers, and cloud architects who want to gain hands-on experience with AWS data services. Prior knowledge of AWS basics and some programming experience (Python/SQL) is recommended but not required. You\u0026rsquo;ll need an OpenWeatherMap API key (free tier available).\nAWS Services You\u0026rsquo;ll Learn Data Collection:\nAWS Lambda - Serverless compute for weather data collection and processing CloudWatch Events - Scheduled execution and automation (hourly/daily) Data Storage:\nAmazon S3 - Scalable object storage for weather data lakes S3 Intelligent Tiering - Cost optimization for data storage Analytics \u0026amp; Visualization:\nAmazon Athena - Interactive query service for S3 weather data Amazon QuickSight - Business intelligence and weather visualization Monitoring \u0026amp; Management:\nAmazon CloudWatch - Monitoring, logging, and weather alerting AWS IAM - Identity and access management SNS - Weather alert notifications External Integration:\nOpenWeatherMap API - Real-time weather data source (Developer Plan) Business Use Cases This weather ETL pipeline demonstrates real-world analytics scenarios:\nClimate Analysis - Temperature, humidity, and pressure trend analysis Agriculture Intelligence - Weather conditions for crop planning and irrigation Tourism Planning - Seasonal weather patterns for travel recommendations Energy Management - Weather-based energy demand forecasting Logistics Optimization - Weather-aware supply chain and transportation Risk Management - Weather alert systems for disaster preparedness Architecture Components Data Sources - OpenWeatherMap API (current weather, forecasts, historical data) Collection Layer - Scheduled Lambda functions for multi-city weather retrieval Processing Layer - Lambda functions for data transformation and enrichment Storage Layer - S3 Data Lake with date/city partitioning Analytics Layer - Athena for SQL querying and QuickSight for weather visualization Monitoring - CloudWatch for logging, metrics, and weather alerting Expected Outcomes By the end of this workshop, you will:\nUnderstand modern serverless data pipeline architectures Master AWS Lambda for real-time weather data collection Build analytics capabilities using live weather data Implement monitoring and weather alert systems Create interactive weather dashboards with QuickSight Integrate external weather APIs into AWS data pipelines Optimize costs with minimal AWS services (under $5/month) Workshop Duration Total Time: 4-6 hours Skill Level: Beginner to Intermediate Cost: Under $5 using AWS Free Tier + OpenWeatherMap Developer Plan Prerequisites Active AWS account with administrative access OpenWeatherMap API key (Developer Plan recommended - 1M calls/month) Basic understanding of cloud computing concepts Familiarity with JSON data format and REST APIs Internet connection to access OpenWeatherMap API Optional: Basic Python or SQL knowledge Workshop Modules Introduction \u0026amp; Architecture Design Weather Data Collection with Lambda Data Processing and Transformation Setting up S3 Data Lake Analytics with Amazon Athena Weather Visualization with QuickSight Cleanup and Next Steps "
},
{
	"uri": "//localhost:1313/2-data-collection-openweathermap/2.2-lambda-weather-collector/",
	"title": "Building Lambda Weather Collector",
	"tags": [],
	"description": "",
	"content": "Building Lambda Weather Collector In this section, we\u0026rsquo;ll create AWS Lambda functions to automatically collect weather data from OpenWeatherMap API and store it in S3. These functions will be the core of our weather data collection system.\nArchitecture Overview graph LR\rA[CloudWatch Events] --\u0026gt; B[Lambda Weather Collector]\rB --\u0026gt; C[OpenWeatherMap API]\rC --\u0026gt; D[API Response]\rD --\u0026gt; B\rB --\u0026gt; E[S3 Weather Data]\rB --\u0026gt; F[CloudWatch Logs]\rG[Parameter Store] --\u0026gt; B\rstyle B fill:#ff9900,stroke:#232f3e,stroke-width:3px\rstyle C fill:#e1f5fe\rstyle E fill:#f3e5f5 Step 1: Create IAM Role for Lambda 1.1 Create Lambda Execution Role Navigate to IAM Console\nAWS Console → IAM → Roles Click \u0026ldquo;Create role\u0026rdquo; Select Trusted Entity\nService: Lambda Click \u0026ldquo;Next\u0026rdquo; Role Configuration\nRole Name: WeatherCollectorLambdaRole Description: Execution role for weather data collection Lambda functions 1.2 Create Custom Policy Policy Name: WeatherCollectorPolicy\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;S3WeatherDataAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:PutObject\u0026#34;, \u0026#34;s3:PutObjectAcl\u0026#34;, \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:ListBucket\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::weather-data-lake-*\u0026#34;, \u0026#34;arn:aws:s3:::weather-data-lake-*/*\u0026#34; ] }, { \u0026#34;Sid\u0026#34;: \u0026#34;ParameterStoreAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [\u0026#34;ssm:GetParameter\u0026#34;, \u0026#34;ssm:GetParameters\u0026#34;], \u0026#34;Resource\u0026#34;: [\u0026#34;arn:aws:ssm:*:*:parameter/weather-etl/*\u0026#34;] }, { \u0026#34;Sid\u0026#34;: \u0026#34;CloudWatchLogsAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:logs:*:*:log-group:/aws/lambda/weather-*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;CloudWatchMetricsAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [\u0026#34;cloudwatch:PutMetricData\u0026#34;], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;cloudwatch:namespace\u0026#34;: \u0026#34;Weather/ETL\u0026#34; } } } ] } 1.3 Attach AWS Managed Policies Also attach this AWS managed policy:\nAWSLambdaBasicExecutionRole: arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole Step 2: Create S3 Bucket for Weather Data 2.1 Create Weather Data Bucket Navigate to S3 Console\nAWS Console → S3 → Create bucket Bucket Configuration\nBucket Name: weather-data-lake-{your-account-id} (replace with your AWS account ID) Region: us-east-1 (or your preferred region) Block Public Access: Keep all settings enabled (recommended) Bucket Structure\nweather-data-lake-123456789012/\r├── raw/\r│ ├── current-weather/\r│ │ └── year=2025/month=01/day=03/hour=10/\r│ └── forecast/\r│ └── year=2025/month=01/day=03/\r└── processed/\r├── current-weather/\r└── forecast/ Step 3: Lambda Function for Current Weather 3.1 Create Current Weather Function Navigate to Lambda Console\nAWS Console → Lambda → Create function Function Configuration\nFunction Name: weather-current-collector Runtime: Python 3.11 Architecture: x86_64 Execution Role: Use existing role → WeatherCollectorLambdaRole 3.2 Function Code File: lambda_function.py\nimport json import boto3 import requests import os from datetime import datetime, timezone from typing import Dict, List, Optional import logging # Configure logging logger = logging.getLogger() logger.setLevel(logging.INFO) # AWS clients s3_client = boto3.client(\u0026#39;s3\u0026#39;) ssm_client = boto3.client(\u0026#39;ssm\u0026#39;) cloudwatch = boto3.client(\u0026#39;cloudwatch\u0026#39;) # Configuration BUCKET_NAME = os.environ.get(\u0026#39;WEATHER_BUCKET_NAME\u0026#39;) API_KEY_PARAMETER = \u0026#39;/weather-etl/openweathermap/api-key\u0026#39; # Target cities for weather collection CITIES = [ {\u0026#39;name\u0026#39;: \u0026#39;Ho Chi Minh City\u0026#39;, \u0026#39;country\u0026#39;: \u0026#39;VN\u0026#39;, \u0026#39;lat\u0026#39;: 10.8231, \u0026#39;lon\u0026#39;: 106.6297}, {\u0026#39;name\u0026#39;: \u0026#39;Hanoi\u0026#39;, \u0026#39;country\u0026#39;: \u0026#39;VN\u0026#39;, \u0026#39;lat\u0026#39;: 21.0285, \u0026#39;lon\u0026#39;: 105.8542}, {\u0026#39;name\u0026#39;: \u0026#39;Singapore\u0026#39;, \u0026#39;country\u0026#39;: \u0026#39;SG\u0026#39;, \u0026#39;lat\u0026#39;: 1.3521, \u0026#39;lon\u0026#39;: 103.8198}, {\u0026#39;name\u0026#39;: \u0026#39;Bangkok\u0026#39;, \u0026#39;country\u0026#39;: \u0026#39;TH\u0026#39;, \u0026#39;lat\u0026#39;: 13.7563, \u0026#39;lon\u0026#39;: 100.5018}, {\u0026#39;name\u0026#39;: \u0026#39;Jakarta\u0026#39;, \u0026#39;country\u0026#39;: \u0026#39;ID\u0026#39;, \u0026#39;lat\u0026#39;: -6.2088, \u0026#39;lon\u0026#39;: 106.8456}, {\u0026#39;name\u0026#39;: \u0026#39;Kuala Lumpur\u0026#39;, \u0026#39;country\u0026#39;: \u0026#39;MY\u0026#39;, \u0026#39;lat\u0026#39;: 3.1390, \u0026#39;lon\u0026#39;: 101.6869} ] def get_api_key() -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;Retrieve OpenWeatherMap API key from Parameter Store.\u0026#34;\u0026#34;\u0026#34; try: response = ssm_client.get_parameter( Name=API_KEY_PARAMETER, WithDecryption=True ) return response[\u0026#39;Parameter\u0026#39;][\u0026#39;Value\u0026#39;] except Exception as e: logger.error(f\u0026#34;Failed to retrieve API key: {e}\u0026#34;) raise def fetch_current_weather(city: Dict, api_key: str) -\u0026gt; Optional[Dict]: \u0026#34;\u0026#34;\u0026#34;Fetch current weather data for a city.\u0026#34;\u0026#34;\u0026#34; base_url = \u0026#34;https://api.openweathermap.org/data/2.5/weather\u0026#34; params = { \u0026#39;lat\u0026#39;: city[\u0026#39;lat\u0026#39;], \u0026#39;lon\u0026#39;: city[\u0026#39;lon\u0026#39;], \u0026#39;appid\u0026#39;: api_key, \u0026#39;units\u0026#39;: \u0026#39;metric\u0026#39; } try: response = requests.get(base_url, params=params, timeout=10) response.raise_for_status() weather_data = response.json() # Add metadata weather_data[\u0026#39;collection_metadata\u0026#39;] = { \u0026#39;collection_time\u0026#39;: datetime.now(timezone.utc).isoformat(), \u0026#39;data_source\u0026#39;: \u0026#39;openweathermap\u0026#39;, \u0026#39;api_version\u0026#39;: \u0026#39;2.5\u0026#39;, \u0026#39;collection_type\u0026#39;: \u0026#39;current_weather\u0026#39; } return weather_data except requests.exceptions.RequestException as e: logger.error(f\u0026#34;Failed to fetch weather for {city[\u0026#39;name\u0026#39;]}: {e}\u0026#34;) return None def store_weather_data(weather_data: Dict, city: Dict) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;Store weather data in S3 with partitioned structure.\u0026#34;\u0026#34;\u0026#34; try: now = datetime.now(timezone.utc) # Create S3 key with partitioning s3_key = ( f\u0026#34;raw/current-weather/\u0026#34; f\u0026#34;year={now.year}/\u0026#34; f\u0026#34;month={now.month:02d}/\u0026#34; f\u0026#34;day={now.day:02d}/\u0026#34; f\u0026#34;hour={now.hour:02d}/\u0026#34; f\u0026#34;{city[\u0026#39;name\u0026#39;].lower().replace(\u0026#39; \u0026#39;, \u0026#39;_\u0026#39;)}_{now.strftime(\u0026#39;%Y%m%d_%H%M%S\u0026#39;)}.json\u0026#34; ) # Upload to S3 s3_client.put_object( Bucket=BUCKET_NAME, Key=s3_key, Body=json.dumps(weather_data, indent=2), ContentType=\u0026#39;application/json\u0026#39;, Metadata={ \u0026#39;city\u0026#39;: city[\u0026#39;name\u0026#39;], \u0026#39;country\u0026#39;: city[\u0026#39;country\u0026#39;], \u0026#39;collection_time\u0026#39;: now.isoformat(), \u0026#39;data_type\u0026#39;: \u0026#39;current_weather\u0026#39; } ) logger.info(f\u0026#34;Stored weather data for {city[\u0026#39;name\u0026#39;]} at s3://{BUCKET_NAME}/{s3_key}\u0026#34;) return True except Exception as e: logger.error(f\u0026#34;Failed to store weather data for {city[\u0026#39;name\u0026#39;]}: {e}\u0026#34;) return False def send_custom_metrics(successful_collections: int, failed_collections: int): \u0026#34;\u0026#34;\u0026#34;Send custom metrics to CloudWatch.\u0026#34;\u0026#34;\u0026#34; try: cloudwatch.put_metric_data( Namespace=\u0026#39;Weather/ETL\u0026#39;, MetricData=[ { \u0026#39;MetricName\u0026#39;: \u0026#39;SuccessfulCollections\u0026#39;, \u0026#39;Value\u0026#39;: successful_collections, \u0026#39;Unit\u0026#39;: \u0026#39;Count\u0026#39;, \u0026#39;Dimensions\u0026#39;: [ { \u0026#39;Name\u0026#39;: \u0026#39;DataType\u0026#39;, \u0026#39;Value\u0026#39;: \u0026#39;CurrentWeather\u0026#39; } ] }, { \u0026#39;MetricName\u0026#39;: \u0026#39;FailedCollections\u0026#39;, \u0026#39;Value\u0026#39;: failed_collections, \u0026#39;Unit\u0026#39;: \u0026#39;Count\u0026#39;, \u0026#39;Dimensions\u0026#39;: [ { \u0026#39;Name\u0026#39;: \u0026#39;DataType\u0026#39;, \u0026#39;Value\u0026#39;: \u0026#39;CurrentWeather\u0026#39; } ] } ] ) except Exception as e: logger.error(f\u0026#34;Failed to send metrics: {e}\u0026#34;) def lambda_handler(event, context): \u0026#34;\u0026#34;\u0026#34;Main Lambda handler for current weather collection.\u0026#34;\u0026#34;\u0026#34; logger.info(\u0026#34;Starting current weather collection\u0026#34;) try: # Get API key api_key = get_api_key() successful_collections = 0 failed_collections = 0 results = [] # Collect weather data for each city for city in CITIES: logger.info(f\u0026#34;Collecting weather data for {city[\u0026#39;name\u0026#39;]}\u0026#34;) # Fetch weather data weather_data = fetch_current_weather(city, api_key) if weather_data: # Store in S3 if store_weather_data(weather_data, city): successful_collections += 1 results.append({ \u0026#39;city\u0026#39;: city[\u0026#39;name\u0026#39;], \u0026#39;status\u0026#39;: \u0026#39;success\u0026#39;, \u0026#39;temperature\u0026#39;: weather_data.get(\u0026#39;main\u0026#39;, {}).get(\u0026#39;temp\u0026#39;), \u0026#39;weather\u0026#39;: weather_data.get(\u0026#39;weather\u0026#39;, [{}])[0].get(\u0026#39;description\u0026#39;) }) else: failed_collections += 1 results.append({ \u0026#39;city\u0026#39;: city[\u0026#39;name\u0026#39;], \u0026#39;status\u0026#39;: \u0026#39;storage_failed\u0026#39; }) else: failed_collections += 1 results.append({ \u0026#39;city\u0026#39;: city[\u0026#39;name\u0026#39;], \u0026#39;status\u0026#39;: \u0026#39;api_failed\u0026#39; }) # Send metrics to CloudWatch send_custom_metrics(successful_collections, failed_collections) # Return results return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: json.dumps({ \u0026#39;message\u0026#39;: \u0026#39;Weather collection completed\u0026#39;, \u0026#39;successful_collections\u0026#39;: successful_collections, \u0026#39;failed_collections\u0026#39;: failed_collections, \u0026#39;results\u0026#39;: results }) } except Exception as e: logger.error(f\u0026#34;Lambda execution failed: {e}\u0026#34;) return { \u0026#39;statusCode\u0026#39;: 500, \u0026#39;body\u0026#39;: json.dumps({ \u0026#39;error\u0026#39;: str(e) }) } 3.3 Environment Variables Set these environment variables for the Lambda function:\nVariable Value WEATHER_BUCKET_NAME weather-data-lake-{your-account-id} 3.4 Function Configuration Timeout: 5 minutes Memory: 256 MB Dead Letter Queue: Create SQS queue weather-collection-dlq Step 4: Lambda Function for Weather Forecasts 4.1 Create Forecast Function Function Name: weather-forecast-collector Runtime: Python 3.11 Execution Role: WeatherCollectorLambdaRole 4.2 Forecast Function Code File: lambda_function.py\nimport json import boto3 import requests import os from datetime import datetime, timezone from typing import Dict, List, Optional import logging # Configure logging logger = logging.getLogger() logger.setLevel(logging.INFO) # AWS clients s3_client = boto3.client(\u0026#39;s3\u0026#39;) ssm_client = boto3.client(\u0026#39;ssm\u0026#39;) cloudwatch = boto3.client(\u0026#39;cloudwatch\u0026#39;) # Configuration BUCKET_NAME = os.environ.get(\u0026#39;WEATHER_BUCKET_NAME\u0026#39;) API_KEY_PARAMETER = \u0026#39;/weather-etl/openweathermap/api-key\u0026#39; # Target cities (same as current weather) CITIES = [ {\u0026#39;name\u0026#39;: \u0026#39;Ho Chi Minh City\u0026#39;, \u0026#39;country\u0026#39;: \u0026#39;VN\u0026#39;, \u0026#39;lat\u0026#39;: 10.8231, \u0026#39;lon\u0026#39;: 106.6297}, {\u0026#39;name\u0026#39;: \u0026#39;Hanoi\u0026#39;, \u0026#39;country\u0026#39;: \u0026#39;VN\u0026#39;, \u0026#39;lat\u0026#39;: 21.0285, \u0026#39;lon\u0026#39;: 105.8542}, {\u0026#39;name\u0026#39;: \u0026#39;Singapore\u0026#39;, \u0026#39;country\u0026#39;: \u0026#39;SG\u0026#39;, \u0026#39;lat\u0026#39;: 1.3521, \u0026#39;lon\u0026#39;: 103.8198}, {\u0026#39;name\u0026#39;: \u0026#39;Bangkok\u0026#39;, \u0026#39;country\u0026#39;: \u0026#39;TH\u0026#39;, \u0026#39;lat\u0026#39;: 13.7563, \u0026#39;lon\u0026#39;: 100.5018}, {\u0026#39;name\u0026#39;: \u0026#39;Jakarta\u0026#39;, \u0026#39;country\u0026#39;: \u0026#39;ID\u0026#39;, \u0026#39;lat\u0026#39;: -6.2088, \u0026#39;lon\u0026#39;: 106.8456}, {\u0026#39;name\u0026#39;: \u0026#39;Kuala Lumpur\u0026#39;, \u0026#39;country\u0026#39;: \u0026#39;MY\u0026#39;, \u0026#39;lat\u0026#39;: 3.1390, \u0026#39;lon\u0026#39;: 101.6869} ] def get_api_key() -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;Retrieve OpenWeatherMap API key from Parameter Store.\u0026#34;\u0026#34;\u0026#34; try: response = ssm_client.get_parameter( Name=API_KEY_PARAMETER, WithDecryption=True ) return response[\u0026#39;Parameter\u0026#39;][\u0026#39;Value\u0026#39;] except Exception as e: logger.error(f\u0026#34;Failed to retrieve API key: {e}\u0026#34;) raise def fetch_weather_forecast(city: Dict, api_key: str) -\u0026gt; Optional[Dict]: \u0026#34;\u0026#34;\u0026#34;Fetch 5-day weather forecast for a city.\u0026#34;\u0026#34;\u0026#34; base_url = \u0026#34;https://api.openweathermap.org/data/2.5/forecast\u0026#34; params = { \u0026#39;lat\u0026#39;: city[\u0026#39;lat\u0026#39;], \u0026#39;lon\u0026#39;: city[\u0026#39;lon\u0026#39;], \u0026#39;appid\u0026#39;: api_key, \u0026#39;units\u0026#39;: \u0026#39;metric\u0026#39; } try: response = requests.get(base_url, params=params, timeout=15) response.raise_for_status() forecast_data = response.json() # Add metadata forecast_data[\u0026#39;collection_metadata\u0026#39;] = { \u0026#39;collection_time\u0026#39;: datetime.now(timezone.utc).isoformat(), \u0026#39;data_source\u0026#39;: \u0026#39;openweathermap\u0026#39;, \u0026#39;api_version\u0026#39;: \u0026#39;2.5\u0026#39;, \u0026#39;collection_type\u0026#39;: \u0026#39;forecast\u0026#39;, \u0026#39;forecast_periods\u0026#39;: len(forecast_data.get(\u0026#39;list\u0026#39;, [])) } return forecast_data except requests.exceptions.RequestException as e: logger.error(f\u0026#34;Failed to fetch forecast for {city[\u0026#39;name\u0026#39;]}: {e}\u0026#34;) return None def store_forecast_data(forecast_data: Dict, city: Dict) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;Store forecast data in S3 with partitioned structure.\u0026#34;\u0026#34;\u0026#34; try: now = datetime.now(timezone.utc) # Create S3 key with partitioning s3_key = ( f\u0026#34;raw/forecast/\u0026#34; f\u0026#34;year={now.year}/\u0026#34; f\u0026#34;month={now.month:02d}/\u0026#34; f\u0026#34;day={now.day:02d}/\u0026#34; f\u0026#34;{city[\u0026#39;name\u0026#39;].lower().replace(\u0026#39; \u0026#39;, \u0026#39;_\u0026#39;)}_{now.strftime(\u0026#39;%Y%m%d_%H%M%S\u0026#39;)}.json\u0026#34; ) # Upload to S3 s3_client.put_object( Bucket=BUCKET_NAME, Key=s3_key, Body=json.dumps(forecast_data, indent=2), ContentType=\u0026#39;application/json\u0026#39;, Metadata={ \u0026#39;city\u0026#39;: city[\u0026#39;name\u0026#39;], \u0026#39;country\u0026#39;: city[\u0026#39;country\u0026#39;], \u0026#39;collection_time\u0026#39;: now.isoformat(), \u0026#39;data_type\u0026#39;: \u0026#39;forecast\u0026#39;, \u0026#39;forecast_periods\u0026#39;: str(len(forecast_data.get(\u0026#39;list\u0026#39;, []))) } ) logger.info(f\u0026#34;Stored forecast data for {city[\u0026#39;name\u0026#39;]} at s3://{BUCKET_NAME}/{s3_key}\u0026#34;) return True except Exception as e: logger.error(f\u0026#34;Failed to store forecast data for {city[\u0026#39;name\u0026#39;]}: {e}\u0026#34;) return False def lambda_handler(event, context): \u0026#34;\u0026#34;\u0026#34;Main Lambda handler for weather forecast collection.\u0026#34;\u0026#34;\u0026#34; logger.info(\u0026#34;Starting weather forecast collection\u0026#34;) try: # Get API key api_key = get_api_key() successful_collections = 0 failed_collections = 0 results = [] # Collect forecast data for each city for city in CITIES: logger.info(f\u0026#34;Collecting forecast data for {city[\u0026#39;name\u0026#39;]}\u0026#34;) # Fetch forecast data forecast_data = fetch_weather_forecast(city, api_key) if forecast_data: # Store in S3 if store_forecast_data(forecast_data, city): successful_collections += 1 forecast_count = len(forecast_data.get(\u0026#39;list\u0026#39;, [])) results.append({ \u0026#39;city\u0026#39;: city[\u0026#39;name\u0026#39;], \u0026#39;status\u0026#39;: \u0026#39;success\u0026#39;, \u0026#39;forecast_periods\u0026#39;: forecast_count }) else: failed_collections += 1 results.append({ \u0026#39;city\u0026#39;: city[\u0026#39;name\u0026#39;], \u0026#39;status\u0026#39;: \u0026#39;storage_failed\u0026#39; }) else: failed_collections += 1 results.append({ \u0026#39;city\u0026#39;: city[\u0026#39;name\u0026#39;], \u0026#39;status\u0026#39;: \u0026#39;api_failed\u0026#39; }) # Send metrics to CloudWatch cloudwatch.put_metric_data( Namespace=\u0026#39;Weather/ETL\u0026#39;, MetricData=[ { \u0026#39;MetricName\u0026#39;: \u0026#39;SuccessfulCollections\u0026#39;, \u0026#39;Value\u0026#39;: successful_collections, \u0026#39;Unit\u0026#39;: \u0026#39;Count\u0026#39;, \u0026#39;Dimensions\u0026#39;: [ { \u0026#39;Name\u0026#39;: \u0026#39;DataType\u0026#39;, \u0026#39;Value\u0026#39;: \u0026#39;Forecast\u0026#39; } ] } ] ) # Return results return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: json.dumps({ \u0026#39;message\u0026#39;: \u0026#39;Forecast collection completed\u0026#39;, \u0026#39;successful_collections\u0026#39;: successful_collections, \u0026#39;failed_collections\u0026#39;: failed_collections, \u0026#39;results\u0026#39;: results }) } except Exception as e: logger.error(f\u0026#34;Lambda execution failed: {e}\u0026#34;) return { \u0026#39;statusCode\u0026#39;: 500, \u0026#39;body\u0026#39;: json.dumps({ \u0026#39;error\u0026#39;: str(e) }) } Step 5: Test Lambda Functions 5.1 Test Current Weather Function Create Test Event\n{ \u0026#34;test\u0026#34;: true, \u0026#34;cities\u0026#34;: [\u0026#34;Ho Chi Minh City\u0026#34;] } Run Test\nClick \u0026ldquo;Test\u0026rdquo; in Lambda console Check execution logs Verify S3 object creation 5.2 Expected Test Results Successful Response:\n{ \u0026#34;statusCode\u0026#34;: 200, \u0026#34;body\u0026#34;: \u0026#34;{\\\u0026#34;message\\\u0026#34;: \\\u0026#34;Weather collection completed\\\u0026#34;, \\\u0026#34;successful_collections\\\u0026#34;: 6, \\\u0026#34;failed_collections\\\u0026#34;: 0, \\\u0026#34;results\\\u0026#34;: [...]}\u0026#34; } S3 Structure After Test:\nweather-data-lake-123456789012/\r└── raw/\r├── current-weather/\r│ └── year=2025/month=01/day=03/hour=14/\r│ ├── ho_chi_minh_city_20250103_140532.json\r│ ├── hanoi_20250103_140534.json\r│ └── ...\r└── forecast/\r└── year=2025/month=01/day=03/\r├── ho_chi_minh_city_20250103_140612.json\r└── ... Step 6: Monitor Function Performance 6.1 CloudWatch Metrics Key metrics to monitor:\nDuration: Function execution time Errors: Failed executions Throttles: Concurrent execution limits Custom Metrics: Successful/failed collections 6.2 Set Up Alarms High Error Rate Alarm:\n{ \u0026#34;AlarmName\u0026#34;: \u0026#34;WeatherCollector-HighErrorRate\u0026#34;, \u0026#34;MetricName\u0026#34;: \u0026#34;Errors\u0026#34;, \u0026#34;Namespace\u0026#34;: \u0026#34;AWS/Lambda\u0026#34;, \u0026#34;Statistic\u0026#34;: \u0026#34;Sum\u0026#34;, \u0026#34;Period\u0026#34;: 300, \u0026#34;EvaluationPeriods\u0026#34;: 2, \u0026#34;Threshold\u0026#34;: 3, \u0026#34;ComparisonOperator\u0026#34;: \u0026#34;GreaterThanThreshold\u0026#34; } Next Steps Excellent! You now have Lambda functions that can collect weather data from OpenWeatherMap API. In the next section, we\u0026rsquo;ll set up automated scheduling using CloudWatch Events to run these functions on a regular schedule.\nWhat You\u0026rsquo;ve Built:\nLambda execution role with proper permissions S3 bucket with partitioned structure Current weather collection function Weather forecast collection function Error handling and monitoring Performance Tips:\nMonitor API response times and adjust timeout accordingly Use CloudWatch Logs Insights to analyze function performance Consider increasing memory for faster execution Set up Dead Letter Queues for failed executions "
},
{
	"uri": "//localhost:1313/2-data-collection-openweathermap/2.1-openweathermap-setup/",
	"title": "OpenWeatherMap API Setup",
	"tags": [],
	"description": "",
	"content": "OpenWeatherMap API Setup In this simplified section, we\u0026rsquo;ll quickly set up an OpenWeatherMap API account to collect weather data for our pipeline.\nStep 1: Sign Up for OpenWeatherMap Create an account Go to https://openweathermap.org and click \u0026ldquo;Sign Up\u0026rdquo; Complete registration with your email Verify your email and log in Step 2: Get Your API Key Access API Keys After logging in, go to \u0026ldquo;API keys\u0026rdquo; section Note your default API key or create a new one named \u0026ldquo;weather-data-collection\u0026rdquo; The free plan includes 1,000 API calls per day and 60 calls per minute - more than enough for our workshop.\nStep 3: Test Your API Key Test your API key with one of these methods:\nBrowser method:\nhttps://api.openweathermap.org/data/2.5/weather?q=London\u0026amp;appid=YOUR_API_KEY cURL method:\ncurl \u0026#34;https://api.openweathermap.org/data/2.5/weather?q=London\u0026amp;appid=YOUR_API_KEY\u0026#34; You should see a JSON response with London weather data.\nStep 4: Store Your API Key Securely Using AWS Systems Manager Open AWS Management Console Go to Systems Manager → Parameter Store Click \u0026ldquo;Create parameter\u0026rdquo; Set these values: Name: /weather-etl/openweathermap/api-key Type: SecureString Value: Your API key Click \u0026ldquo;Create parameter\u0026rdquo; Key API Endpoints We\u0026rsquo;ll Use # Current Weather https://api.openweathermap.org/data/2.5/weather?q={city}\u0026amp;appid={API key} # 5-Day Forecast (3-hour intervals) https://api.openweathermap.org/data/2.5/forecast?q={city}\u0026amp;appid={API key} Next Steps That\u0026rsquo;s it! You now have a working OpenWeatherMap API key securely stored in Parameter Store. In the next section, we\u0026rsquo;ll create a Lambda function to collect weather data.\nCompleted:\nCreated OpenWeatherMap account Obtained API key Stored API key in AWS Parameter Store "
},
{
	"uri": "//localhost:1313/2-data-collection-openweathermap/2.3-automated-scheduling/",
	"title": "Automated Scheduling with CloudWatch Events",
	"tags": [],
	"description": "",
	"content": "Automated Scheduling with CloudWatch Events In this section, we\u0026rsquo;ll set up automated scheduling for our weather data collection using Amazon CloudWatch Events (now called Amazon EventBridge). This will ensure our Lambda functions run on a regular schedule to collect weather data consistently.\nArchitecture Overview graph TD\rA[CloudWatch Events Rules] --\u0026gt; B[Current Weather Schedule\u0026lt;br/\u0026gt;Every Hour]\rA --\u0026gt; C[Forecast Schedule\u0026lt;br/\u0026gt;Every 6 Hours]\rB --\u0026gt; D[Lambda: weather-current-collector]\rC --\u0026gt; E[Lambda: weather-forecast-collector]\rD --\u0026gt; F[S3: Current Weather Data]\rE --\u0026gt; G[S3: Forecast Data]\rH[CloudWatch Alarms] --\u0026gt; I[SNS Notifications]\rstyle A fill:#e8f5e8\rstyle D fill:#ff9900,stroke:#232f3e,stroke-width:3px\rstyle E fill:#ff9900,stroke:#232f3e,stroke-width:3px Step 1: Create CloudWatch Events Rules 1.1 Schedule for Current Weather Collection Navigate to CloudWatch Console\nAWS Console → CloudWatch → Events → Rules Click \u0026ldquo;Create rule\u0026rdquo; Event Source Configuration\nEvent Source: Schedule Schedule Expression: rate(1 hour) Description: \u0026ldquo;Trigger current weather collection every hour\u0026rdquo; Target Configuration\nTarget: Lambda function Function: weather-current-collector Configure input: Constant (JSON text) Input JSON:\n{ \u0026#34;source\u0026#34;: \u0026#34;cloudwatch-events\u0026#34;, \u0026#34;detail-type\u0026#34;: \u0026#34;Scheduled Event\u0026#34;, \u0026#34;detail\u0026#34;: { \u0026#34;collection_type\u0026#34;: \u0026#34;current_weather\u0026#34;, \u0026#34;scheduled_time\u0026#34;: \u0026#34;hourly\u0026#34; } } Rule Details Name: weather-current-hourly Description: \u0026ldquo;Hourly collection of current weather data for 6 cities\u0026rdquo; State: Enabled 1.2 Schedule for Weather Forecast Collection Create Second Rule\nEvent Source: Schedule Schedule Expression: rate(6 hours) Description: \u0026ldquo;Trigger weather forecast collection every 6 hours\u0026rdquo; Target Configuration\nTarget: Lambda function Function: weather-forecast-collector Input JSON:\n{ \u0026#34;source\u0026#34;: \u0026#34;cloudwatch-events\u0026#34;, \u0026#34;detail-type\u0026#34;: \u0026#34;Scheduled Event\u0026#34;, \u0026#34;detail\u0026#34;: { \u0026#34;collection_type\u0026#34;: \u0026#34;forecast\u0026#34;, \u0026#34;scheduled_time\u0026#34;: \u0026#34;every_6_hours\u0026#34; } } Rule Details Name: weather-forecast-6hourly Description: \u0026ldquo;6-hourly collection of weather forecast data\u0026rdquo; State: Enabled Step 2: Advanced Scheduling Options 2.1 Using Cron Expressions For more precise scheduling, you can use cron expressions:\nCurrent Weather at specific times:\ncron(0 0,6,12,18 * * ? *) This runs at 00:00, 06:00, 12:00, and 18:00 UTC daily.\nForecast at 06:00 and 18:00 UTC:\ncron(0 6,18 * * ? *) Hourly during business hours (UTC+7):\ncron(0 1-14 * * ? *) This runs hourly from 01:00 to 14:00 UTC (08:00 to 21:00 Vietnam time).\n2.2 Different Schedules for Different Cities Create city-specific rules:\nRule for Asian Cities (UTC+7/+8):\n{ \u0026#34;source\u0026#34;: \u0026#34;cloudwatch-events\u0026#34;, \u0026#34;detail\u0026#34;: { \u0026#34;cities\u0026#34;: [\u0026#34;Ho Chi Minh City\u0026#34;, \u0026#34;Hanoi\u0026#34;, \u0026#34;Singapore\u0026#34;, \u0026#34;Bangkok\u0026#34;], \u0026#34;timezone\u0026#34;: \u0026#34;Asia/Ho_Chi_Minh\u0026#34; } } Schedule: cron(0 1,7,13,19 * * ? *) (Peak hours in Asia)\nStep 3: Create SNS Topic for Notifications 3.1 Set Up SNS Topic Navigate to SNS Console\nAWS Console → SNS → Topics Click \u0026ldquo;Create topic\u0026rdquo; Topic Configuration\nType: Standard Name: weather-collection-alerts Display Name: \u0026ldquo;Weather Collection Alerts\u0026rdquo; Create Subscription\nProtocol: Email Endpoint: Your email address Confirm subscription via email 3.2 SNS Topic Policy { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;AllowCloudWatchAlarmsToPublish\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;cloudwatch.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;SNS:Publish\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:sns:us-east-1:123456789012:weather-collection-alerts\u0026#34; } ] } Step 4: Set Up Monitoring and Alarms 4.1 Lambda Function Alarms Error Rate Alarm:\n{ \u0026#34;AlarmName\u0026#34;: \u0026#34;WeatherCollection-HighErrorRate\u0026#34;, \u0026#34;AlarmDescription\u0026#34;: \u0026#34;High error rate in weather collection functions\u0026#34;, \u0026#34;MetricName\u0026#34;: \u0026#34;Errors\u0026#34;, \u0026#34;Namespace\u0026#34;: \u0026#34;AWS/Lambda\u0026#34;, \u0026#34;Statistic\u0026#34;: \u0026#34;Sum\u0026#34;, \u0026#34;Period\u0026#34;: 3600, \u0026#34;EvaluationPeriods\u0026#34;: 1, \u0026#34;Threshold\u0026#34;: 2, \u0026#34;ComparisonOperator\u0026#34;: \u0026#34;GreaterThanOrEqualToThreshold\u0026#34;, \u0026#34;Dimensions\u0026#34;: [ { \u0026#34;Name\u0026#34;: \u0026#34;FunctionName\u0026#34;, \u0026#34;Value\u0026#34;: \u0026#34;weather-current-collector\u0026#34; } ], \u0026#34;AlarmActions\u0026#34;: [ \u0026#34;arn:aws:sns:us-east-1:123456789012:weather-collection-alerts\u0026#34; ] } Duration Alarm:\n{ \u0026#34;AlarmName\u0026#34;: \u0026#34;WeatherCollection-LongDuration\u0026#34;, \u0026#34;AlarmDescription\u0026#34;: \u0026#34;Weather collection taking too long\u0026#34;, \u0026#34;MetricName\u0026#34;: \u0026#34;Duration\u0026#34;, \u0026#34;Namespace\u0026#34;: \u0026#34;AWS/Lambda\u0026#34;, \u0026#34;Statistic\u0026#34;: \u0026#34;Average\u0026#34;, \u0026#34;Period\u0026#34;: 3600, \u0026#34;EvaluationPeriods\u0026#34;: 2, \u0026#34;Threshold\u0026#34;: 30000, \u0026#34;ComparisonOperator\u0026#34;: \u0026#34;GreaterThanThreshold\u0026#34;, \u0026#34;Unit\u0026#34;: \u0026#34;Milliseconds\u0026#34; } 4.2 Custom Metrics Alarms Failed Collections Alarm:\n{ \u0026#34;AlarmName\u0026#34;: \u0026#34;WeatherCollection-FailedCollections\u0026#34;, \u0026#34;AlarmDescription\u0026#34;: \u0026#34;Too many failed weather data collections\u0026#34;, \u0026#34;MetricName\u0026#34;: \u0026#34;FailedCollections\u0026#34;, \u0026#34;Namespace\u0026#34;: \u0026#34;Weather/ETL\u0026#34;, \u0026#34;Statistic\u0026#34;: \u0026#34;Sum\u0026#34;, \u0026#34;Period\u0026#34;: 3600, \u0026#34;EvaluationPeriods\u0026#34;: 1, \u0026#34;Threshold\u0026#34;: 3, \u0026#34;ComparisonOperator\u0026#34;: \u0026#34;GreaterThanThreshold\u0026#34;, \u0026#34;AlarmActions\u0026#34;: [ \u0026#34;arn:aws:sns:us-east-1:123456789012:weather-collection-alerts\u0026#34; ] } 4.3 API Usage Monitoring Create a custom metric to track OpenWeatherMap API usage:\nLambda Function Addition:\ndef send_api_usage_metrics(api_calls_made: int): \u0026#34;\u0026#34;\u0026#34;Track API usage to monitor limits.\u0026#34;\u0026#34;\u0026#34; try: cloudwatch.put_metric_data( Namespace=\u0026#39;Weather/APIUsage\u0026#39;, MetricData=[ { \u0026#39;MetricName\u0026#39;: \u0026#39;APICalls\u0026#39;, \u0026#39;Value\u0026#39;: api_calls_made, \u0026#39;Unit\u0026#39;: \u0026#39;Count\u0026#39;, \u0026#39;Dimensions\u0026#39;: [ { \u0026#39;Name\u0026#39;: \u0026#39;Provider\u0026#39;, \u0026#39;Value\u0026#39;: \u0026#39;OpenWeatherMap\u0026#39; } ] } ] ) except Exception as e: logger.error(f\u0026#34;Failed to send API usage metrics: {e}\u0026#34;) API Limit Alarm:\n{ \u0026#34;AlarmName\u0026#34;: \u0026#34;OpenWeatherMap-APILimitApproaching\u0026#34;, \u0026#34;AlarmDescription\u0026#34;: \u0026#34;Approaching OpenWeatherMap daily API limit\u0026#34;, \u0026#34;MetricName\u0026#34;: \u0026#34;APICalls\u0026#34;, \u0026#34;Namespace\u0026#34;: \u0026#34;Weather/APIUsage\u0026#34;, \u0026#34;Statistic\u0026#34;: \u0026#34;Sum\u0026#34;, \u0026#34;Period\u0026#34;: 86400, \u0026#34;EvaluationPeriods\u0026#34;: 1, \u0026#34;Threshold\u0026#34;: 800, \u0026#34;ComparisonOperator\u0026#34;: \u0026#34;GreaterThanThreshold\u0026#34; } Step 5: Create CloudWatch Dashboard 5.1 Weather Collection Dashboard Navigate to CloudWatch Dashboards\nAWS Console → CloudWatch → Dashboards Click \u0026ldquo;Create dashboard\u0026rdquo; Dashboard Configuration\nName: Weather-ETL-Dashboard Add Widgets\nWidget 1: Lambda Invocations\n{ \u0026#34;type\u0026#34;: \u0026#34;metric\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;metrics\u0026#34;: [ [ \u0026#34;AWS/Lambda\u0026#34;, \u0026#34;Invocations\u0026#34;, \u0026#34;FunctionName\u0026#34;, \u0026#34;weather-current-collector\u0026#34; ], [\u0026#34;.\u0026#34;, \u0026#34;.\u0026#34;, \u0026#34;.\u0026#34;, \u0026#34;weather-forecast-collector\u0026#34;] ], \u0026#34;period\u0026#34;: 3600, \u0026#34;stat\u0026#34;: \u0026#34;Sum\u0026#34;, \u0026#34;region\u0026#34;: \u0026#34;us-east-1\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;Lambda Invocations\u0026#34; } } Widget 2: Collection Success Rate\n{ \u0026#34;type\u0026#34;: \u0026#34;metric\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;metrics\u0026#34;: [ [\u0026#34;Weather/ETL\u0026#34;, \u0026#34;SuccessfulCollections\u0026#34;, \u0026#34;DataType\u0026#34;, \u0026#34;CurrentWeather\u0026#34;], [\u0026#34;.\u0026#34;, \u0026#34;FailedCollections\u0026#34;, \u0026#34;.\u0026#34;, \u0026#34;.\u0026#34;], [\u0026#34;.\u0026#34;, \u0026#34;SuccessfulCollections\u0026#34;, \u0026#34;.\u0026#34;, \u0026#34;Forecast\u0026#34;], [\u0026#34;.\u0026#34;, \u0026#34;FailedCollections\u0026#34;, \u0026#34;.\u0026#34;, \u0026#34;.\u0026#34;] ], \u0026#34;period\u0026#34;: 3600, \u0026#34;stat\u0026#34;: \u0026#34;Sum\u0026#34;, \u0026#34;region\u0026#34;: \u0026#34;us-east-1\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;Collection Success vs Failures\u0026#34; } } Widget 3: API Usage\n{ \u0026#34;type\u0026#34;: \u0026#34;metric\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;metrics\u0026#34;: [[\u0026#34;Weather/APIUsage\u0026#34;, \u0026#34;APICalls\u0026#34;, \u0026#34;Provider\u0026#34;, \u0026#34;OpenWeatherMap\u0026#34;]], \u0026#34;period\u0026#34;: 86400, \u0026#34;stat\u0026#34;: \u0026#34;Sum\u0026#34;, \u0026#34;region\u0026#34;: \u0026#34;us-east-1\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;Daily API Usage\u0026#34; } } Step 6: Error Handling and Recovery 6.1 Dead Letter Queues Create DLQ for Current Weather\nAWS Console → SQS → Create queue Name: weather-current-collector-dlq Type: Standard queue Configure Lambda DLQ\nLambda function → Configuration → Asynchronous invocation Dead letter queue: weather-current-collector-dlq Maximum age of event: 6 hours Retry attempts: 2 6.2 DLQ Processing Function Create DLQ processor: weather-dlq-processor\nimport json import boto3 import logging from datetime import datetime logger = logging.getLogger() logger.setLevel(logging.INFO) def lambda_handler(event, context): \u0026#34;\u0026#34;\u0026#34;Process messages from dead letter queue.\u0026#34;\u0026#34;\u0026#34; for record in event[\u0026#39;Records\u0026#39;]: try: # Parse the failed message message_body = json.loads(record[\u0026#39;body\u0026#39;]) logger.error(f\u0026#34;Processing failed message: {message_body}\u0026#34;) # You can implement retry logic here # Or send to another system for manual review # Send alert to SNS sns = boto3.client(\u0026#39;sns\u0026#39;) sns.publish( TopicArn=\u0026#39;arn:aws:sns:us-east-1:123456789012:weather-collection-alerts\u0026#39;, Message=f\u0026#34;Weather collection failed permanently: {json.dumps(message_body)}\u0026#34;, Subject=\u0026#34;Weather Collection - Dead Letter Queue Alert\u0026#34; ) except Exception as e: logger.error(f\u0026#34;Failed to process DLQ message: {e}\u0026#34;) return {\u0026#39;statusCode\u0026#39;: 200} Step 7: Cost Optimization 7.1 Optimize Scheduling Peak Hours Only (for development):\ncron(0 6,12,18 * * ? *) Reduces from 24 to 3 collections per day.\nBusiness Hours Only:\ncron(0 8-17 * * 1-5 *) Collects only during weekday business hours.\n7.2 Batch Collection Modify Lambda to collect multiple cities in fewer API calls:\ndef collect_weather_batch(cities_batch: List[Dict], api_key: str): \u0026#34;\u0026#34;\u0026#34;Collect weather for multiple cities in one function call.\u0026#34;\u0026#34;\u0026#34; results = [] for city in cities_batch: weather_data = fetch_current_weather(city, api_key) if weather_data: store_weather_data(weather_data, city) results.append({\u0026#39;city\u0026#39;: city[\u0026#39;name\u0026#39;], \u0026#39;status\u0026#39;: \u0026#39;success\u0026#39;}) # Small delay to respect rate limits time.sleep(0.1) return results 7.3 Smart Scheduling Weather-Dependent Scheduling:\ndef should_collect_forecast(current_weather: Dict) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;Decide if forecast collection is needed based on current conditions.\u0026#34;\u0026#34;\u0026#34; # Skip forecast if weather is stable weather_main = current_weather.get(\u0026#39;weather\u0026#39;, [{}])[0].get(\u0026#39;main\u0026#39;, \u0026#39;\u0026#39;) if weather_main in [\u0026#39;Clear\u0026#39;, \u0026#39;Clouds\u0026#39;] and \\ current_weather.get(\u0026#39;wind\u0026#39;, {}).get(\u0026#39;speed\u0026#39;, 0) \u0026lt; 5: return False # Skip forecast for stable weather return True Step 8: Testing Scheduled Executions 8.1 Manual Testing Test Current Weather Rule:\naws events put-events \\ --entries Source=weather.test,DetailType=\u0026#34;Manual Test\u0026#34;,Detail=\u0026#39;{\u0026#34;test\u0026#34;: true}\u0026#39; Verify S3 Data:\naws s3 ls s3://weather-data-lake-123456789012/raw/current-weather/ --recursive 8.2 Schedule Validation Check Rule Status:\naws events describe-rule --name weather-current-hourly List Targets:\naws events list-targets-by-rule --rule weather-current-hourly Troubleshooting Common Issues Issue 1: Lambda Not Triggered Check:\nRule state is \u0026ldquo;Enabled\u0026rdquo; Lambda permissions allow EventBridge Target configuration is correct Solution:\naws lambda add-permission \\ --function-name weather-current-collector \\ --statement-id allow-eventbridge \\ --action lambda:InvokeFunction \\ --principal events.amazonaws.com \\ --source-arn arn:aws:events:us-east-1:123456789012:rule/weather-current-hourly Issue 2: High API Usage Monitor Daily Usage:\ndef check_daily_api_usage(): \u0026#34;\u0026#34;\u0026#34;Check current API usage against limits.\u0026#34;\u0026#34;\u0026#34; # Query CloudWatch for daily API calls cloudwatch = boto3.client(\u0026#39;cloudwatch\u0026#39;) response = cloudwatch.get_metric_statistics( Namespace=\u0026#39;Weather/APIUsage\u0026#39;, MetricName=\u0026#39;APICalls\u0026#39;, Dimensions=[{\u0026#39;Name\u0026#39;: \u0026#39;Provider\u0026#39;, \u0026#39;Value\u0026#39;: \u0026#39;OpenWeatherMap\u0026#39;}], StartTime=datetime.now().replace(hour=0, minute=0, second=0), EndTime=datetime.now(), Period=86400, Statistics=[\u0026#39;Sum\u0026#39;] ) if response[\u0026#39;Datapoints\u0026#39;]: daily_usage = response[\u0026#39;Datapoints\u0026#39;][0][\u0026#39;Sum\u0026#39;] if daily_usage \u0026gt; 800: # 80% of 1000 limit # Disable collection or reduce frequency pass Next Steps Perfect! You now have automated weather data collection running on a schedule. In the next section, we\u0026rsquo;ll test the entire system and set up comprehensive monitoring to ensure everything works smoothly.\nWhat You\u0026rsquo;ve Accomplished:\nAutomated hourly current weather collection 6-hourly weather forecast collection CloudWatch monitoring and alarms SNS notifications for failures Dead letter queue for error handling Cost optimization strategies Optimization Tips:\nMonitor CloudWatch costs as metrics can add up Use CloudWatch Logs Insights for troubleshooting Consider using EventBridge rules for more complex scheduling Set up proper alerting thresholds based on your needs "
},
{
	"uri": "//localhost:1313/2-data-collection-openweathermap/2.4-testing-monitoring/",
	"title": "Testing and Monitoring Weather Collection",
	"tags": [],
	"description": "",
	"content": "Testing and Monitoring Weather Collection Now that we have our Lambda functions and automated scheduling configured, it\u0026rsquo;s time to thoroughly test the weather collection pipeline and set up comprehensive monitoring. We\u0026rsquo;ll validate data quality, performance, and ensure the system runs reliably.\nTesting Strategy Overview We\u0026rsquo;ll test our weather collection system using:\nManual Function Testing - Direct Lambda invocation Scheduled Execution Testing - CloudWatch Events validation Data Quality Validation - S3 data verification Performance Testing - Load and timing analysis Error Scenario Testing - Failure handling validation graph LR\rA[Manual Tests] --\u0026gt; B[Lambda Functions]\rC[Scheduled Tests] --\u0026gt; D[CloudWatch Events]\rE[Data Quality Tests] --\u0026gt; F[S3 Storage]\rG[Performance Tests] --\u0026gt; H[CloudWatch Metrics]\rI[Error Tests] --\u0026gt; J[Error Handling]\rstyle B fill:#ff9900,stroke:#232f3e,stroke-width:3px\rstyle F fill:#f3e5f5\rstyle H fill:#e8f5e8 Step 1: Manual Function Testing 1.1 Test Current Weather Function Navigate to Lambda Console\nAWS Console → Lambda → Functions Click weather-current-collector Create Test Event\n{ \u0026#34;source\u0026#34;: \u0026#34;manual-test\u0026#34;, \u0026#34;detail-type\u0026#34;: \u0026#34;Manual Test Event\u0026#34;, \u0026#34;detail\u0026#34;: { \u0026#34;test_mode\u0026#34;: true, \u0026#34;cities_to_test\u0026#34;: [\u0026#34;Ho Chi Minh City\u0026#34;, \u0026#34;Singapore\u0026#34;] } } Run Test\nClick \u0026ldquo;Test\u0026rdquo; button Wait for execution completion Review execution results Expected Response:\n{ \u0026#34;statusCode\u0026#34;: 200, \u0026#34;body\u0026#34;: \u0026#34;{\\\u0026#34;message\\\u0026#34;: \\\u0026#34;Weather collection completed\\\u0026#34;, \\\u0026#34;successful_collections\\\u0026#34;: 6, \\\u0026#34;failed_collections\\\u0026#34;: 0, \\\u0026#34;results\\\u0026#34;: [...]}\u0026#34; } 1.2 Test Weather Forecast Function Navigate to forecast function\nClick weather-forecast-collector Create Test Event\n{ \u0026#34;source\u0026#34;: \u0026#34;manual-test\u0026#34;, \u0026#34;detail-type\u0026#34;: \u0026#34;Manual Test Event\u0026#34;, \u0026#34;detail\u0026#34;: { \u0026#34;test_mode\u0026#34;: true, \u0026#34;include_extended_forecast\u0026#34;: true } } Verify Test Results\nCheck execution logs in CloudWatch Verify S3 objects are created Validate JSON structure Step 2: Data Quality Validation 2.1 S3 Data Structure Verification Check S3 Bucket Structure:\naws s3 ls s3://weather-data-lake-123456789012/ --recursive Expected Structure:\nweather-data-lake-123456789012/\r├── raw/\r│ ├── current-weather/\r│ │ └── year=2025/month=01/day=03/hour=14/\r│ │ ├── ho_chi_minh_city_20250103_140532.json\r│ │ ├── hanoi_20250103_140534.json\r│ │ ├── singapore_20250103_140536.json\r│ │ ├── bangkok_20250103_140538.json\r│ │ ├── jakarta_20250103_140540.json\r│ │ └── kuala_lumpur_20250103_140542.json\r│ └── forecast/\r│ └── year=2025/month=01/day=03/\r│ ├── ho_chi_minh_city_20250103_140612.json\r│ └── ... 2.2 Data Quality Validation Script File: validate_weather_data.py\nimport boto3 import json import pandas as pd from datetime import datetime, timedelta from typing import Dict, List, Optional import logging logging.basicConfig(level=logging.INFO) logger = logging.getLogger(__name__) class WeatherDataValidator: def __init__(self, bucket_name: str): self.s3 = boto3.client(\u0026#39;s3\u0026#39;) self.bucket_name = bucket_name def get_recent_files(self, data_type: str = \u0026#39;current-weather\u0026#39;, hours_back: int = 24) -\u0026gt; List[str]: \u0026#34;\u0026#34;\u0026#34;Get weather data files from the last N hours.\u0026#34;\u0026#34;\u0026#34; files = [] now = datetime.utcnow() for hour_offset in range(hours_back): check_time = now - timedelta(hours=hour_offset) prefix = f\u0026#34;raw/{data_type}/year={check_time.year}/month={check_time.month:02d}/day={check_time.day:02d}/\u0026#34; if data_type == \u0026#39;current-weather\u0026#39;: prefix += f\u0026#34;hour={check_time.hour:02d}/\u0026#34; try: response = self.s3.list_objects_v2( Bucket=self.bucket_name, Prefix=prefix ) for obj in response.get(\u0026#39;Contents\u0026#39;, []): files.append(obj[\u0026#39;Key\u0026#39;]) except Exception as e: logger.warning(f\u0026#34;Could not list objects for {prefix}: {e}\u0026#34;) return files def validate_current_weather_file(self, file_key: str) -\u0026gt; Dict: \u0026#34;\u0026#34;\u0026#34;Validate a current weather data file.\u0026#34;\u0026#34;\u0026#34; try: response = self.s3.get_object(Bucket=self.bucket_name, Key=file_key) data = json.loads(response[\u0026#39;Body\u0026#39;].read()) validation_results = { \u0026#39;file_key\u0026#39;: file_key, \u0026#39;valid\u0026#39;: True, \u0026#39;errors\u0026#39;: [], \u0026#39;warnings\u0026#39;: [] } # Required fields validation required_fields = [ \u0026#39;coord\u0026#39;, \u0026#39;weather\u0026#39;, \u0026#39;main\u0026#39;, \u0026#39;wind\u0026#39;, \u0026#39;clouds\u0026#39;, \u0026#39;dt\u0026#39;, \u0026#39;sys\u0026#39;, \u0026#39;timezone\u0026#39;, \u0026#39;id\u0026#39;, \u0026#39;name\u0026#39;, \u0026#39;cod\u0026#39; ] for field in required_fields: if field not in data: validation_results[\u0026#39;errors\u0026#39;].append(f\u0026#34;Missing required field: {field}\u0026#34;) validation_results[\u0026#39;valid\u0026#39;] = False # Temperature validation if \u0026#39;main\u0026#39; in data and \u0026#39;temp\u0026#39; in data[\u0026#39;main\u0026#39;]: temp = data[\u0026#39;main\u0026#39;][\u0026#39;temp\u0026#39;] if not (-50 \u0026lt;= temp \u0026lt;= 60): # Reasonable temperature range in Celsius validation_results[\u0026#39;warnings\u0026#39;].append(f\u0026#34;Temperature seems unusual: {temp}°C\u0026#34;) # Humidity validation if \u0026#39;main\u0026#39; in data and \u0026#39;humidity\u0026#39; in data[\u0026#39;main\u0026#39;]: humidity = data[\u0026#39;main\u0026#39;][\u0026#39;humidity\u0026#39;] if not (0 \u0026lt;= humidity \u0026lt;= 100): validation_results[\u0026#39;errors\u0026#39;].append(f\u0026#34;Invalid humidity value: {humidity}%\u0026#34;) validation_results[\u0026#39;valid\u0026#39;] = False # Collection metadata validation if \u0026#39;collection_metadata\u0026#39; in data: metadata = data[\u0026#39;collection_metadata\u0026#39;] required_metadata = [\u0026#39;collection_time\u0026#39;, \u0026#39;data_source\u0026#39;, \u0026#39;api_version\u0026#39;] for field in required_metadata: if field not in metadata: validation_results[\u0026#39;warnings\u0026#39;].append(f\u0026#34;Missing metadata field: {field}\u0026#34;) else: validation_results[\u0026#39;warnings\u0026#39;].append(\u0026#34;Missing collection_metadata\u0026#34;) # Data freshness check if \u0026#39;dt\u0026#39; in data: data_timestamp = datetime.fromtimestamp(data[\u0026#39;dt\u0026#39;]) age_hours = (datetime.utcnow() - data_timestamp).total_seconds() / 3600 if age_hours \u0026gt; 2: # Data older than 2 hours validation_results[\u0026#39;warnings\u0026#39;].append(f\u0026#34;Data is {age_hours:.1f} hours old\u0026#34;) return validation_results except Exception as e: return { \u0026#39;file_key\u0026#39;: file_key, \u0026#39;valid\u0026#39;: False, \u0026#39;errors\u0026#39;: [f\u0026#34;Failed to parse file: {str(e)}\u0026#34;], \u0026#39;warnings\u0026#39;: [] } def validate_forecast_file(self, file_key: str) -\u0026gt; Dict: \u0026#34;\u0026#34;\u0026#34;Validate a forecast data file.\u0026#34;\u0026#34;\u0026#34; try: response = self.s3.get_object(Bucket=self.bucket_name, Key=file_key) data = json.loads(response[\u0026#39;Body\u0026#39;].read()) validation_results = { \u0026#39;file_key\u0026#39;: file_key, \u0026#39;valid\u0026#39;: True, \u0026#39;errors\u0026#39;: [], \u0026#39;warnings\u0026#39;: [] } # Required fields for forecast required_fields = [\u0026#39;cod\u0026#39;, \u0026#39;message\u0026#39;, \u0026#39;cnt\u0026#39;, \u0026#39;list\u0026#39;, \u0026#39;city\u0026#39;] for field in required_fields: if field not in data: validation_results[\u0026#39;errors\u0026#39;].append(f\u0026#34;Missing required field: {field}\u0026#34;) validation_results[\u0026#39;valid\u0026#39;] = False # Validate forecast list if \u0026#39;list\u0026#39; in data: forecast_list = data[\u0026#39;list\u0026#39;] if len(forecast_list) == 0: validation_results[\u0026#39;errors\u0026#39;].append(\u0026#34;Empty forecast list\u0026#34;) validation_results[\u0026#39;valid\u0026#39;] = False # Check first few forecast items for i, forecast_item in enumerate(forecast_list[:3]): required_item_fields = [\u0026#39;dt\u0026#39;, \u0026#39;main\u0026#39;, \u0026#39;weather\u0026#39;, \u0026#39;wind\u0026#39;] for field in required_item_fields: if field not in forecast_item: validation_results[\u0026#39;errors\u0026#39;].append( f\u0026#34;Forecast item {i} missing field: {field}\u0026#34; ) validation_results[\u0026#39;valid\u0026#39;] = False return validation_results except Exception as e: return { \u0026#39;file_key\u0026#39;: file_key, \u0026#39;valid\u0026#39;: False, \u0026#39;errors\u0026#39;: [f\u0026#34;Failed to parse file: {str(e)}\u0026#34;], \u0026#39;warnings\u0026#39;: [] } def generate_validation_report(self) -\u0026gt; Dict: \u0026#34;\u0026#34;\u0026#34;Generate comprehensive validation report.\u0026#34;\u0026#34;\u0026#34; report = { \u0026#39;timestamp\u0026#39;: datetime.utcnow().isoformat(), \u0026#39;current_weather\u0026#39;: { \u0026#39;files_checked\u0026#39;: 0, \u0026#39;valid_files\u0026#39;: 0, \u0026#39;invalid_files\u0026#39;: 0, \u0026#39;total_errors\u0026#39;: 0, \u0026#39;total_warnings\u0026#39;: 0, \u0026#39;details\u0026#39;: [] }, \u0026#39;forecast\u0026#39;: { \u0026#39;files_checked\u0026#39;: 0, \u0026#39;valid_files\u0026#39;: 0, \u0026#39;invalid_files\u0026#39;: 0, \u0026#39;total_errors\u0026#39;: 0, \u0026#39;total_warnings\u0026#39;: 0, \u0026#39;details\u0026#39;: [] } } # Validate current weather files logger.info(\u0026#34;Validating current weather files...\u0026#34;) current_files = self.get_recent_files(\u0026#39;current-weather\u0026#39;, 6) # Last 6 hours for file_key in current_files: result = self.validate_current_weather_file(file_key) report[\u0026#39;current_weather\u0026#39;][\u0026#39;files_checked\u0026#39;] += 1 report[\u0026#39;current_weather\u0026#39;][\u0026#39;details\u0026#39;].append(result) if result[\u0026#39;valid\u0026#39;]: report[\u0026#39;current_weather\u0026#39;][\u0026#39;valid_files\u0026#39;] += 1 else: report[\u0026#39;current_weather\u0026#39;][\u0026#39;invalid_files\u0026#39;] += 1 report[\u0026#39;current_weather\u0026#39;][\u0026#39;total_errors\u0026#39;] += len(result[\u0026#39;errors\u0026#39;]) report[\u0026#39;current_weather\u0026#39;][\u0026#39;total_warnings\u0026#39;] += len(result[\u0026#39;warnings\u0026#39;]) # Validate forecast files logger.info(\u0026#34;Validating forecast files...\u0026#34;) forecast_files = self.get_recent_files(\u0026#39;forecast\u0026#39;, 24) # Last 24 hours for file_key in forecast_files: result = self.validate_forecast_file(file_key) report[\u0026#39;forecast\u0026#39;][\u0026#39;files_checked\u0026#39;] += 1 report[\u0026#39;forecast\u0026#39;][\u0026#39;details\u0026#39;].append(result) if result[\u0026#39;valid\u0026#39;]: report[\u0026#39;forecast\u0026#39;][\u0026#39;valid_files\u0026#39;] += 1 else: report[\u0026#39;forecast\u0026#39;][\u0026#39;invalid_files\u0026#39;] += 1 report[\u0026#39;forecast\u0026#39;][\u0026#39;total_errors\u0026#39;] += len(result[\u0026#39;errors\u0026#39;]) report[\u0026#39;forecast\u0026#39;][\u0026#39;total_warnings\u0026#39;] += len(result[\u0026#39;warnings\u0026#39;]) return report def main(): # Initialize validator validator = WeatherDataValidator(\u0026#39;weather-data-lake-123456789012\u0026#39;) # Replace with your bucket # Generate validation report report = validator.generate_validation_report() # Print summary print(\u0026#34;WEATHER DATA VALIDATION REPORT\u0026#34;) print(\u0026#34;=\u0026#34; * 50) print(f\u0026#34;Report Generated: {report[\u0026#39;timestamp\u0026#39;]}\u0026#34;) print() print(\u0026#34;CURRENT WEATHER DATA:\u0026#34;) print(f\u0026#34; Files Checked: {report[\u0026#39;current_weather\u0026#39;][\u0026#39;files_checked\u0026#39;]}\u0026#34;) print(f\u0026#34; Valid Files: {report[\u0026#39;current_weather\u0026#39;][\u0026#39;valid_files\u0026#39;]}\u0026#34;) print(f\u0026#34; Invalid Files: {report[\u0026#39;current_weather\u0026#39;][\u0026#39;invalid_files\u0026#39;]}\u0026#34;) print(f\u0026#34; Total Errors: {report[\u0026#39;current_weather\u0026#39;][\u0026#39;total_errors\u0026#39;]}\u0026#34;) print(f\u0026#34; Total Warnings: {report[\u0026#39;current_weather\u0026#39;][\u0026#39;total_warnings\u0026#39;]}\u0026#34;) print(\u0026#34;\\nFORECAST DATA:\u0026#34;) print(f\u0026#34; Files Checked: {report[\u0026#39;forecast\u0026#39;][\u0026#39;files_checked\u0026#39;]}\u0026#34;) print(f\u0026#34; Valid Files: {report[\u0026#39;forecast\u0026#39;][\u0026#39;valid_files\u0026#39;]}\u0026#34;) print(f\u0026#34; Invalid Files: {report[\u0026#39;forecast\u0026#39;][\u0026#39;invalid_files\u0026#39;]}\u0026#34;) print(f\u0026#34; Total Errors: {report[\u0026#39;forecast\u0026#39;][\u0026#39;total_errors\u0026#39;]}\u0026#34;) print(f\u0026#34; Total Warnings: {report[\u0026#39;forecast\u0026#39;][\u0026#39;total_warnings\u0026#39;]}\u0026#34;) # Show first few errors/warnings print(\u0026#34;\\nDETAILED ISSUES:\u0026#34;) for data_type in [\u0026#39;current_weather\u0026#39;, \u0026#39;forecast\u0026#39;]: for detail in report[data_type][\u0026#39;details\u0026#39;][:3]: # Show first 3 if detail[\u0026#39;errors\u0026#39;] or detail[\u0026#39;warnings\u0026#39;]: print(f\u0026#34;\\nFile: {detail[\u0026#39;file_key\u0026#39;]}\u0026#34;) for error in detail[\u0026#39;errors\u0026#39;]: print(f\u0026#34; ERROR: {error}\u0026#34;) for warning in detail[\u0026#39;warnings\u0026#39;]: print(f\u0026#34; WARNING: {warning}\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: main() Step 3: Performance Testing 3.1 Lambda Performance Analysis Create Performance Monitoring Script: monitor_lambda_performance.py\nimport boto3 import pandas as pd from datetime import datetime, timedelta import matplotlib.pyplot as plt import seaborn as sns class LambdaPerformanceMonitor: def __init__(self, region: str = \u0026#39;us-east-1\u0026#39;): self.cloudwatch = boto3.client(\u0026#39;cloudwatch\u0026#39;, region_name=region) self.lambda_client = boto3.client(\u0026#39;lambda\u0026#39;, region_name=region) def get_lambda_metrics(self, function_name: str, hours_back: int = 24) -\u0026gt; pd.DataFrame: \u0026#34;\u0026#34;\u0026#34;Get Lambda performance metrics.\u0026#34;\u0026#34;\u0026#34; end_time = datetime.utcnow() start_time = end_time - timedelta(hours=hours_back) metrics = [ \u0026#39;Duration\u0026#39;, \u0026#39;Errors\u0026#39;, \u0026#39;Invocations\u0026#39;, \u0026#39;Throttles\u0026#39;, \u0026#39;ConcurrentExecutions\u0026#39; ] all_data = [] for metric in metrics: try: response = self.cloudwatch.get_metric_statistics( Namespace=\u0026#39;AWS/Lambda\u0026#39;, MetricName=metric, Dimensions=[ { \u0026#39;Name\u0026#39;: \u0026#39;FunctionName\u0026#39;, \u0026#39;Value\u0026#39;: function_name } ], StartTime=start_time, EndTime=end_time, Period=3600, # 1 hour periods Statistics=[\u0026#39;Average\u0026#39;, \u0026#39;Maximum\u0026#39;, \u0026#39;Sum\u0026#39;] ) for datapoint in response.get(\u0026#39;Datapoints\u0026#39;, []): all_data.append({ \u0026#39;Timestamp\u0026#39;: datapoint[\u0026#39;Timestamp\u0026#39;], \u0026#39;MetricName\u0026#39;: metric, \u0026#39;Average\u0026#39;: datapoint.get(\u0026#39;Average\u0026#39;, 0), \u0026#39;Maximum\u0026#39;: datapoint.get(\u0026#39;Maximum\u0026#39;, 0), \u0026#39;Sum\u0026#39;: datapoint.get(\u0026#39;Sum\u0026#39;, 0) }) except Exception as e: print(f\u0026#34;Error getting metric {metric}: {e}\u0026#34;) return pd.DataFrame(all_data) def analyze_performance(self, function_name: str) -\u0026gt; Dict: \u0026#34;\u0026#34;\u0026#34;Analyze Lambda function performance.\u0026#34;\u0026#34;\u0026#34; df = self.get_lambda_metrics(function_name, 24) if df.empty: return {\u0026#39;error\u0026#39;: \u0026#39;No metrics data available\u0026#39;} analysis = { \u0026#39;function_name\u0026#39;: function_name, \u0026#39;analysis_time\u0026#39;: datetime.utcnow().isoformat(), \u0026#39;metrics\u0026#39;: {} } # Duration analysis duration_data = df[df[\u0026#39;MetricName\u0026#39;] == \u0026#39;Duration\u0026#39;] if not duration_data.empty: analysis[\u0026#39;metrics\u0026#39;][\u0026#39;duration\u0026#39;] = { \u0026#39;avg_duration_ms\u0026#39;: duration_data[\u0026#39;Average\u0026#39;].mean(), \u0026#39;max_duration_ms\u0026#39;: duration_data[\u0026#39;Maximum\u0026#39;].max(), \u0026#39;performance_trend\u0026#39;: \u0026#39;stable\u0026#39; if duration_data[\u0026#39;Average\u0026#39;].std() \u0026lt; 1000 else \u0026#39;variable\u0026#39; } # Error analysis error_data = df[df[\u0026#39;MetricName\u0026#39;] == \u0026#39;Errors\u0026#39;] total_errors = error_data[\u0026#39;Sum\u0026#39;].sum() if not error_data.empty else 0 invocation_data = df[df[\u0026#39;MetricName\u0026#39;] == \u0026#39;Invocations\u0026#39;] total_invocations = invocation_data[\u0026#39;Sum\u0026#39;].sum() if not invocation_data.empty else 0 error_rate = (total_errors / total_invocations * 100) if total_invocations \u0026gt; 0 else 0 analysis[\u0026#39;metrics\u0026#39;][\u0026#39;reliability\u0026#39;] = { \u0026#39;total_invocations\u0026#39;: total_invocations, \u0026#39;total_errors\u0026#39;: total_errors, \u0026#39;error_rate_percent\u0026#39;: error_rate, \u0026#39;reliability_status\u0026#39;: \u0026#39;good\u0026#39; if error_rate \u0026lt; 1 else \u0026#39;needs_attention\u0026#39; } # Throttling analysis throttle_data = df[df[\u0026#39;MetricName\u0026#39;] == \u0026#39;Throttles\u0026#39;] total_throttles = throttle_data[\u0026#39;Sum\u0026#39;].sum() if not throttle_data.empty else 0 analysis[\u0026#39;metrics\u0026#39;][\u0026#39;scaling\u0026#39;] = { \u0026#39;total_throttles\u0026#39;: total_throttles, \u0026#39;throttling_status\u0026#39;: \u0026#39;none\u0026#39; if total_throttles == 0 else \u0026#39;occurring\u0026#39; } return analysis def test_lambda_performance(): \u0026#34;\u0026#34;\u0026#34;Run performance tests on weather collection functions.\u0026#34;\u0026#34;\u0026#34; monitor = LambdaPerformanceMonitor() functions = [\u0026#39;weather-current-collector\u0026#39;, \u0026#39;weather-forecast-collector\u0026#39;] print(\u0026#34;LAMBDA PERFORMANCE ANALYSIS\u0026#34;) print(\u0026#34;=\u0026#34; * 50) for function_name in functions: print(f\u0026#34;\\nAnalyzing: {function_name}\u0026#34;) analysis = monitor.analyze_performance(function_name) if \u0026#39;error\u0026#39; in analysis: print(f\u0026#34; Error: {analysis[\u0026#39;error\u0026#39;]}\u0026#34;) continue # Duration metrics if \u0026#39;duration\u0026#39; in analysis[\u0026#39;metrics\u0026#39;]: duration = analysis[\u0026#39;metrics\u0026#39;][\u0026#39;duration\u0026#39;] print(f\u0026#34; Average Duration: {duration[\u0026#39;avg_duration_ms\u0026#39;]:.0f}ms\u0026#34;) print(f\u0026#34; Max Duration: {duration[\u0026#39;max_duration_ms\u0026#39;]:.0f}ms\u0026#34;) print(f\u0026#34; Performance: {duration[\u0026#39;performance_trend\u0026#39;]}\u0026#34;) # Reliability metrics if \u0026#39;reliability\u0026#39; in analysis[\u0026#39;metrics\u0026#39;]: reliability = analysis[\u0026#39;metrics\u0026#39;][\u0026#39;reliability\u0026#39;] print(f\u0026#34; Total Invocations: {reliability[\u0026#39;total_invocations\u0026#39;]}\u0026#34;) print(f\u0026#34; Error Rate: {reliability[\u0026#39;error_rate_percent\u0026#39;]:.2f}%\u0026#34;) print(f\u0026#34; Status: {reliability[\u0026#39;reliability_status\u0026#39;]}\u0026#34;) # Scaling metrics if \u0026#39;scaling\u0026#39; in analysis[\u0026#39;metrics\u0026#39;]: scaling = analysis[\u0026#39;metrics\u0026#39;][\u0026#39;scaling\u0026#39;] print(f\u0026#34; Throttles: {scaling[\u0026#39;total_throttles\u0026#39;]}\u0026#34;) print(f\u0026#34; Scaling Status: {scaling[\u0026#39;throttling_status\u0026#39;]}\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: test_lambda_performance() 3.2 API Usage Tracking Create API monitoring script: track_api_usage.py\nimport boto3 from datetime import datetime, timedelta def check_api_usage_limits(): \u0026#34;\u0026#34;\u0026#34;Monitor OpenWeatherMap API usage against limits.\u0026#34;\u0026#34;\u0026#34; cloudwatch = boto3.client(\u0026#39;cloudwatch\u0026#39;) # Get API usage for last 24 hours end_time = datetime.utcnow() start_time = end_time - timedelta(hours=24) try: response = cloudwatch.get_metric_statistics( Namespace=\u0026#39;Weather/APIUsage\u0026#39;, MetricName=\u0026#39;APICalls\u0026#39;, Dimensions=[ { \u0026#39;Name\u0026#39;: \u0026#39;Provider\u0026#39;, \u0026#39;Value\u0026#39;: \u0026#39;OpenWeatherMap\u0026#39; } ], StartTime=start_time, EndTime=end_time, Period=86400, # Daily period Statistics=[\u0026#39;Sum\u0026#39;] ) daily_usage = 0 if response[\u0026#39;Datapoints\u0026#39;]: daily_usage = response[\u0026#39;Datapoints\u0026#39;][0][\u0026#39;Sum\u0026#39;] # OpenWeatherMap free tier: 1000 calls/day usage_percentage = (daily_usage / 1000) * 100 print(\u0026#34;API USAGE MONITORING\u0026#34;) print(\u0026#34;=\u0026#34; * 30) print(f\u0026#34;24-hour API calls: {daily_usage}\u0026#34;) print(f\u0026#34;Daily limit: 1,000\u0026#34;) print(f\u0026#34;Usage: {usage_percentage:.1f}%\u0026#34;) if usage_percentage \u0026gt; 80: print(\u0026#34;⚠️ WARNING: Approaching daily limit!\u0026#34;) elif usage_percentage \u0026gt; 95: print(\u0026#34;🚨 CRITICAL: Very close to daily limit!\u0026#34;) else: print(\u0026#34;✅ Usage within safe limits\u0026#34;) # Calculate hourly rate hourly_rate = daily_usage / 24 projected_daily = hourly_rate * 24 print(f\u0026#34;\\nProjected daily usage: {projected_daily:.0f} calls\u0026#34;) print(f\u0026#34;Average hourly rate: {hourly_rate:.1f} calls/hour\u0026#34;) return { \u0026#39;daily_usage\u0026#39;: daily_usage, \u0026#39;usage_percentage\u0026#39;: usage_percentage, \u0026#39;hourly_rate\u0026#39;: hourly_rate } except Exception as e: print(f\u0026#34;Error checking API usage: {e}\u0026#34;) return None if __name__ == \u0026#34;__main__\u0026#34;: check_api_usage_limits() Step 4: Error Scenario Testing 4.1 Test API Key Failure Create test with invalid API key:\nTemporarily modify API key in Parameter Store\naws ssm put-parameter \\ --name \u0026#34;/weather-etl/openweathermap/api-key\u0026#34; \\ --value \u0026#34;invalid-key-for-testing\u0026#34; \\ --type \u0026#34;SecureString\u0026#34; \\ --overwrite Trigger Lambda function manually\nVerify error handling and CloudWatch logs\nRestore correct API key\n4.2 Test Network Failure Simulation Create network failure test script: test_error_scenarios.py\nimport boto3 import json from datetime import datetime def test_lambda_error_handling(): \u0026#34;\u0026#34;\u0026#34;Test error handling in Lambda functions.\u0026#34;\u0026#34;\u0026#34; lambda_client = boto3.client(\u0026#39;lambda\u0026#39;) # Test cases test_cases = [ { \u0026#39;name\u0026#39;: \u0026#39;API Timeout Simulation\u0026#39;, \u0026#39;payload\u0026#39;: { \u0026#39;test_scenario\u0026#39;: \u0026#39;api_timeout\u0026#39;, \u0026#39;cities\u0026#39;: [\u0026#39;Ho Chi Minh City\u0026#39;] } }, { \u0026#39;name\u0026#39;: \u0026#39;Invalid City Test\u0026#39;, \u0026#39;payload\u0026#39;: { \u0026#39;test_scenario\u0026#39;: \u0026#39;invalid_city\u0026#39;, \u0026#39;cities\u0026#39;: [\u0026#39;NonExistentCity\u0026#39;] } }, { \u0026#39;name\u0026#39;: \u0026#39;S3 Permission Test\u0026#39;, \u0026#39;payload\u0026#39;: { \u0026#39;test_scenario\u0026#39;: \u0026#39;s3_permission_test\u0026#39;, \u0026#39;cities\u0026#39;: [\u0026#39;Singapore\u0026#39;] } } ] functions = [\u0026#39;weather-current-collector\u0026#39;, \u0026#39;weather-forecast-collector\u0026#39;] print(\u0026#34;ERROR SCENARIO TESTING\u0026#34;) print(\u0026#34;=\u0026#34; * 40) for function_name in functions: print(f\u0026#34;\\nTesting function: {function_name}\u0026#34;) for test_case in test_cases: print(f\u0026#34; Running: {test_case[\u0026#39;name\u0026#39;]}\u0026#34;) try: response = lambda_client.invoke( FunctionName=function_name, InvocationType=\u0026#39;RequestResponse\u0026#39;, Payload=json.dumps(test_case[\u0026#39;payload\u0026#39;]) ) payload = json.loads(response[\u0026#39;Payload\u0026#39;].read()) status_code = payload.get(\u0026#39;statusCode\u0026#39;, 500) if status_code == 200: print(\u0026#34; ✅ Handled gracefully\u0026#34;) else: print(\u0026#34; ⚠️ Error occurred (expected for error tests)\u0026#34;) except Exception as e: print(f\u0026#34; ❌ Test failed: {e}\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: test_lambda_error_handling() Step 5: Comprehensive Monitoring Setup 5.1 Create CloudWatch Dashboard Dashboard JSON for comprehensive monitoring:\n{ \u0026#34;widgets\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;metric\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;metrics\u0026#34;: [ [ \u0026#34;AWS/Lambda\u0026#34;, \u0026#34;Invocations\u0026#34;, \u0026#34;FunctionName\u0026#34;, \u0026#34;weather-current-collector\u0026#34; ], [\u0026#34;.\u0026#34;, \u0026#34;Errors\u0026#34;, \u0026#34;.\u0026#34;, \u0026#34;.\u0026#34;], [\u0026#34;.\u0026#34;, \u0026#34;Duration\u0026#34;, \u0026#34;.\u0026#34;, \u0026#34;.\u0026#34;], [\u0026#34;.\u0026#34;, \u0026#34;Invocations\u0026#34;, \u0026#34;.\u0026#34;, \u0026#34;weather-forecast-collector\u0026#34;], [\u0026#34;.\u0026#34;, \u0026#34;Errors\u0026#34;, \u0026#34;.\u0026#34;, \u0026#34;.\u0026#34;], [\u0026#34;.\u0026#34;, \u0026#34;Duration\u0026#34;, \u0026#34;.\u0026#34;, \u0026#34;.\u0026#34;] ], \u0026#34;period\u0026#34;: 3600, \u0026#34;stat\u0026#34;: \u0026#34;Sum\u0026#34;, \u0026#34;region\u0026#34;: \u0026#34;us-east-1\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;Lambda Function Metrics\u0026#34; } }, { \u0026#34;type\u0026#34;: \u0026#34;metric\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;metrics\u0026#34;: [ [ \u0026#34;Weather/ETL\u0026#34;, \u0026#34;SuccessfulCollections\u0026#34;, \u0026#34;DataType\u0026#34;, \u0026#34;CurrentWeather\u0026#34; ], [\u0026#34;.\u0026#34;, \u0026#34;FailedCollections\u0026#34;, \u0026#34;.\u0026#34;, \u0026#34;.\u0026#34;], [\u0026#34;.\u0026#34;, \u0026#34;SuccessfulCollections\u0026#34;, \u0026#34;.\u0026#34;, \u0026#34;Forecast\u0026#34;], [\u0026#34;.\u0026#34;, \u0026#34;FailedCollections\u0026#34;, \u0026#34;.\u0026#34;, \u0026#34;.\u0026#34;] ], \u0026#34;period\u0026#34;: 3600, \u0026#34;stat\u0026#34;: \u0026#34;Sum\u0026#34;, \u0026#34;region\u0026#34;: \u0026#34;us-east-1\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;Collection Success Metrics\u0026#34; } }, { \u0026#34;type\u0026#34;: \u0026#34;metric\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;metrics\u0026#34;: [ [\u0026#34;Weather/APIUsage\u0026#34;, \u0026#34;APICalls\u0026#34;, \u0026#34;Provider\u0026#34;, \u0026#34;OpenWeatherMap\u0026#34;] ], \u0026#34;period\u0026#34;: 86400, \u0026#34;stat\u0026#34;: \u0026#34;Sum\u0026#34;, \u0026#34;region\u0026#34;: \u0026#34;us-east-1\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;Daily API Usage\u0026#34; } } ] } 5.2 Set Up Automated Testing Create scheduled validation function: weather-data-validator\nimport boto3 import json from datetime import datetime, timedelta def lambda_handler(event, context): \u0026#34;\u0026#34;\u0026#34;Scheduled data validation function.\u0026#34;\u0026#34;\u0026#34; # This function runs daily to validate data quality validator = WeatherDataValidator(\u0026#39;weather-data-lake-123456789012\u0026#39;) report = validator.generate_validation_report() # Check for issues total_errors = (report[\u0026#39;current_weather\u0026#39;][\u0026#39;total_errors\u0026#39;] + report[\u0026#39;forecast\u0026#39;][\u0026#39;total_errors\u0026#39;]) if total_errors \u0026gt; 0: # Send alert to SNS sns = boto3.client(\u0026#39;sns\u0026#39;) sns.publish( TopicArn=\u0026#39;arn:aws:sns:us-east-1:123456789012:weather-collection-alerts\u0026#39;, Subject=\u0026#39;Weather Data Quality Issues Detected\u0026#39;, Message=f\u0026#34;Data validation found {total_errors} errors. Please check the dashboard.\u0026#34; ) # Store validation report in S3 s3 = boto3.client(\u0026#39;s3\u0026#39;) report_key = f\u0026#34;validation-reports/{datetime.utcnow().strftime(\u0026#39;%Y/%m/%d\u0026#39;)}/validation-report.json\u0026#34; s3.put_object( Bucket=\u0026#39;weather-data-lake-123456789012\u0026#39;, Key=report_key, Body=json.dumps(report, indent=2), ContentType=\u0026#39;application/json\u0026#39; ) return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: json.dumps({ \u0026#39;validation_completed\u0026#39;: True, \u0026#39;total_errors\u0026#39;: total_errors, \u0026#39;report_location\u0026#39;: f\u0026#34;s3://weather-data-lake-123456789012/{report_key}\u0026#34; }) } Step 6: Cost Monitoring 6.1 Daily Cost Tracking Create cost monitoring script: monitor_daily_costs.py\nimport boto3 from datetime import datetime, timedelta def estimate_weather_collection_costs(): \u0026#34;\u0026#34;\u0026#34;Estimate daily costs for weather collection system.\u0026#34;\u0026#34;\u0026#34; # Get CloudWatch metrics for cost estimation cloudwatch = boto3.client(\u0026#39;cloudwatch\u0026#39;) end_time = datetime.utcnow() start_time = end_time - timedelta(hours=24) costs = { \u0026#39;lambda_invocations\u0026#39;: 0, \u0026#39;lambda_duration\u0026#39;: 0, \u0026#39;api_calls\u0026#39;: 0, \u0026#39;s3_storage\u0026#39;: 0, \u0026#39;cloudwatch_metrics\u0026#39;: 0, \u0026#39;total_estimated\u0026#39;: 0 } try: # Lambda invocations cost for function_name in [\u0026#39;weather-current-collector\u0026#39;, \u0026#39;weather-forecast-collector\u0026#39;]: response = cloudwatch.get_metric_statistics( Namespace=\u0026#39;AWS/Lambda\u0026#39;, MetricName=\u0026#39;Invocations\u0026#39;, Dimensions=[{\u0026#39;Name\u0026#39;: \u0026#39;FunctionName\u0026#39;, \u0026#39;Value\u0026#39;: function_name}], StartTime=start_time, EndTime=end_time, Period=86400, Statistics=[\u0026#39;Sum\u0026#39;] ) if response[\u0026#39;Datapoints\u0026#39;]: invocations = response[\u0026#39;Datapoints\u0026#39;][0][\u0026#39;Sum\u0026#39;] costs[\u0026#39;lambda_invocations\u0026#39;] += invocations * 0.0000002 # $0.20 per 1M requests # Lambda duration cost for function_name in [\u0026#39;weather-current-collector\u0026#39;, \u0026#39;weather-forecast-collector\u0026#39;]: response = cloudwatch.get_metric_statistics( Namespace=\u0026#39;AWS/Lambda\u0026#39;, MetricName=\u0026#39;Duration\u0026#39;, Dimensions=[{\u0026#39;Name\u0026#39;: \u0026#39;FunctionName\u0026#39;, \u0026#39;Value\u0026#39;: function_name}], StartTime=start_time, EndTime=end_time, Period=86400, Statistics=[\u0026#39;Sum\u0026#39;] ) if response[\u0026#39;Datapoints\u0026#39;]: total_duration_ms = response[\u0026#39;Datapoints\u0026#39;][0][\u0026#39;Sum\u0026#39;] gb_seconds = (total_duration_ms / 1000) * (256 / 1024) # 256MB memory costs[\u0026#39;lambda_duration\u0026#39;] += gb_seconds * 0.0000166667 # $0.0000166667 per GB-second # Estimate S3 storage cost (approximate) # Each weather file is ~2KB, 6 cities * 24 hours = 144 files/day for current weather # 6 cities * 4 times/day = 24 files/day for forecast daily_files = (6 * 24) + (6 * 4) # 168 files daily_storage_gb = (daily_files * 2) / (1024 * 1024) # Convert KB to GB costs[\u0026#39;s3_storage\u0026#39;] = daily_storage_gb * 0.023 # $0.023 per GB/month, daily portion # OpenWeatherMap API is free tier (up to 1000 calls/day) costs[\u0026#39;api_calls\u0026#39;] = 0 # CloudWatch metrics cost (approximate) # Custom metrics: $0.30 per metric per month custom_metrics = 3 # APICalls, SuccessfulCollections, FailedCollections costs[\u0026#39;cloudwatch_metrics\u0026#39;] = (custom_metrics * 0.30) / 30 # Daily portion costs[\u0026#39;total_estimated\u0026#39;] = sum(costs.values()) print(\u0026#34;DAILY COST ESTIMATION\u0026#34;) print(\u0026#34;=\u0026#34; * 30) print(f\u0026#34;Lambda Invocations: ${costs[\u0026#39;lambda_invocations\u0026#39;]:.6f}\u0026#34;) print(f\u0026#34;Lambda Duration: ${costs[\u0026#39;lambda_duration\u0026#39;]:.6f}\u0026#34;) print(f\u0026#34;S3 Storage: ${costs[\u0026#39;s3_storage\u0026#39;]:.6f}\u0026#34;) print(f\u0026#34;API Calls: ${costs[\u0026#39;api_calls\u0026#39;]:.6f} (Free Tier)\u0026#34;) print(f\u0026#34;CloudWatch Metrics: ${costs[\u0026#39;cloudwatch_metrics\u0026#39;]:.6f}\u0026#34;) print(f\u0026#34;Total Daily Cost: ${costs[\u0026#39;total_estimated\u0026#39;]:.6f}\u0026#34;) print(f\u0026#34;Monthly Projection: ${costs[\u0026#39;total_estimated\u0026#39;] * 30:.2f}\u0026#34;) return costs except Exception as e: print(f\u0026#34;Error calculating costs: {e}\u0026#34;) return None if __name__ == \u0026#34;__main__\u0026#34;: estimate_weather_collection_costs() Step 7: Testing Results Summary Expected Test Results ✅ Successful Test Indicators:\nLambda Functions:\nCurrent weather function executes in \u0026lt; 30 seconds Forecast function executes in \u0026lt; 45 seconds Error rate \u0026lt; 1% No throttling issues Data Quality:\nAll required fields present in JSON Temperature values within reasonable range (-50°C to +60°C) Humidity values 0-100% Timestamps within expected range Scheduling:\nCloudWatch Events trigger correctly Functions execute on schedule No missed executions Cost Efficiency:\nDaily cost \u0026lt; $0.10 Monthly projection \u0026lt; $3.00 API usage \u0026lt; 500 calls/day (50% of free tier) Troubleshooting Common Issues Issue 1: Lambda Timeout\nIncrease timeout to 5 minutes Check network connectivity Optimize API request handling Issue 2: Invalid Weather Data\nVerify API key validity Check city coordinates Validate JSON parsing Issue 3: S3 Permission Errors\nReview IAM role permissions Check bucket policy Verify bucket exists Issue 4: High API Usage\nReduce collection frequency Implement smart caching Monitor daily limits Next Steps Excellent! You now have a fully tested weather data collection system with comprehensive monitoring. The system is ready for production use.\nTesting Completed:\n✅ Manual function testing ✅ Data quality validation ✅ Performance monitoring ✅ Error scenario testing ✅ Cost tracking ✅ Automated monitoring setup Production Monitoring:\nSet up daily validation reports Monitor API usage trends Review cost projections weekly Test error scenarios monthly Update city lists as needed In the next module, we\u0026rsquo;ll build serverless data processing with Lambda to transform and analyze this weather data!\n"
},
{
	"uri": "//localhost:1313/1-introduction/",
	"title": "Introduction and Architecture",
	"tags": [],
	"description": "",
	"content": "Building a Serverless Weather ETL Pipeline Welcome to this hands-on workshop where you\u0026rsquo;ll build a complete Extract, Transform, Load (ETL) pipeline using AWS serverless services to collect, process, and visualize weather data.\nWorkshop Overview In this workshop, you\u0026rsquo;ll create a simplified but complete weather data pipeline that demonstrates core ETL concepts using AWS serverless technologies. The workshop is designed to be completed in 2-3 hours with an estimated cost of under $10 for the entire experience.\nLearning Objectives By the end of this workshop, you will:\nBuild a serverless data collection system using AWS Lambda Implement data transformation and processing workflows Store and query data using Amazon S3 and Athena Create visualizations with Amazon QuickSight Apply AWS best practices for cost optimization and resource cleanup Architecture Overview Our weather ETL pipeline follows this simplified serverless architecture:\nOpenWeatherMap API → Lambda Collector → S3 Raw Data → Lambda Processor → S3 Processed Data → Athena Analytics → QuickSight Dashboard Key Components:\nData Source: OpenWeatherMap API for real-time weather data Collection: AWS Lambda function to fetch weather data Storage: Amazon S3 for both raw and processed data Processing: AWS Lambda for data transformation Analytics: Amazon Athena for SQL queries Visualization: Amazon QuickSight for dashboards Workshop Modules This workshop is organized into 6 modules:\nModule 1: Introduction and Architecture Workshop overview and learning objectives Architecture design and AWS services introduction Prerequisites and setup requirements Module 2: Weather Data Collection with OpenWeatherMap Setting up OpenWeatherMap API account Creating Lambda function for data collection Configuring automated data retrieval Testing and monitoring the collection process Module 3: Serverless Data Processing with Lambda Building data transformation Lambda function Converting raw weather JSON to analytical format Implementing data validation and enrichment Setting up processing triggers Module 4: Data Analysis with Amazon Athena Creating S3 data lake structure Setting up Athena tables and schemas Writing SQL queries for weather analysis Exploring data patterns and insights Module 5: Data Visualization with QuickSight Setting up Amazon QuickSight Creating weather dashboards Building interactive visualizations Sharing and publishing dashboards Module 6: Resource Cleanup and Next Steps Comprehensive cleanup checklist Cost optimization strategies Suggested improvements and extensions Additional learning resources Prerequisites Before starting this workshop, ensure you have:\nAWS Account with administrative access Basic familiarity with AWS console Understanding of basic programming concepts OpenWeatherMap account (free tier sufficient) Estimated Costs This workshop is designed to be cost-effective:\nOpenWeatherMap API: Free (up to 1,000 calls/day) AWS Lambda: ~$1-2 (well within free tier) Amazon S3: ~$1-2 for storage Amazon Athena: ~$2-3 for queries Amazon QuickSight: ~$3-4 (30-day free trial available) Total estimated cost: Under $10 for the complete workshop\nGetting Started Ready to begin? Let\u0026rsquo;s start with Module 2: Weather Data Collection with OpenWeatherMap where you\u0026rsquo;ll set up your data source and create your first Lambda function.\nNote: Remember to follow the cleanup procedures in Module 6 to avoid ongoing charges after completing the workshop.\n"
},
{
	"uri": "//localhost:1313/2-data-collection-openweathermap/",
	"title": "Weather Data Collection with Lambda",
	"tags": [],
	"description": "",
	"content": "Weather Data Collection with Lambda In this module, you\u0026rsquo;ll build an automated weather data collection system using AWS Lambda functions and the OpenWeatherMap API. This serverless approach provides a scalable, cost-effective solution for gathering weather data from multiple cities on a scheduled basis.\nModule Learning Objectives By the end of this module, you will:\nSet up OpenWeatherMap API access and understand rate limits Create Lambda functions to collect current weather and forecast data Implement automated scheduling using CloudWatch Events Store weather data in S3 with proper partitioning Set up monitoring and error handling Test the complete data collection pipeline Architecture Overview graph TB A[CloudWatch Events\u0026lt;br/\u0026gt;Scheduler] --\u0026gt; B[Lambda Functions] B --\u0026gt; C[OpenWeatherMap API] C --\u0026gt; D[Weather Data Response] D --\u0026gt; B B --\u0026gt; E[S3 Data Lake\u0026lt;br/\u0026gt;Partitioned Storage] B --\u0026gt; F[CloudWatch Metrics] G[Parameter Store\u0026lt;br/\u0026gt;API Keys] --\u0026gt; B H[SNS Alerts] --\u0026gt; I[Email Notifications] F --\u0026gt; H style B fill:#ff9900,stroke:#232f3e,stroke-width:3px style E fill:#f3e5f5 style C fill:#e1f5fe What You\u0026rsquo;ll Build Data Collection Components Lambda Functions: Serverless data collectors for current weather and forecasts CloudWatch Events: Automated scheduling (hourly for current weather, 6-hourly for forecasts) S3 Storage: Organized data lake with year/month/day/hour partitioning Parameter Store: Secure API key management Monitoring and Alerting CloudWatch Metrics: Custom metrics for collection success/failure rates CloudWatch Alarms: Automated alerts for system issues SNS Notifications: Email alerts for failures and API limit warnings Data Sources Current Weather: Real-time conditions for 6 Southeast Asian cities 5-Day Forecasts: Weather predictions with 3-hour intervals API Metadata: Collection timestamps and data quality indicators Target Cities The workshop collects weather data for these cities:\nCity Country Coordinates Timezone Ho Chi Minh City Vietnam 10.8231, 106.6297 UTC+7 Hanoi Vietnam 21.0285, 105.8542 UTC+7 Singapore Singapore 1.3521, 103.8198 UTC+8 Bangkok Thailand 13.7563, 100.5018 UTC+7 Jakarta Indonesia -6.2088, 106.8456 UTC+7 Kuala Lumpur Malaysia 3.1390, 101.6869 UTC+8 Expected Data Volume Daily Collection Estimates Current Weather: 6 cities × 24 hours = 144 files/day (~288 KB) Weather Forecasts: 6 cities × 4 collections = 24 files/day (~480 KB) Total Daily Storage: ~768 KB (~23 MB/month) API Usage Current Weather: 144 API calls/day Forecasts: 24 API calls/day Total Daily API Calls: 168 (well within 1,000 free tier limit) Cost Breakdown Estimated Monthly Costs Lambda Invocations: ~$0.01 (168 daily invocations) Lambda Duration: ~$0.15 (average 5-second executions) S3 Storage: ~$0.01 (23 MB monthly storage) CloudWatch Metrics: ~$0.90 (custom metrics) API Calls: $0.00 (free tier) Total Module Cost: ~$1.07/month\nModule Structure This module is divided into four sections:\n2.1 OpenWeatherMap Setup Create OpenWeatherMap account Obtain API key and understand limits Test API endpoints Store credentials securely 2.2 Building Lambda Weather Collector Create IAM roles and policies Develop Lambda functions for data collection Implement error handling and retry logic Configure environment variables 2.3 Automated Scheduling Set up CloudWatch Events rules Configure different schedules for different data types Implement monitoring and alerting Create SNS topics for notifications 2.4 Testing and Monitoring Test Lambda functions manually Validate data quality and structure Set up comprehensive monitoring Perform load and error testing Key Technologies Used AWS Services AWS Lambda: Serverless compute for data collection Amazon S3: Scalable object storage for data lake Amazon CloudWatch: Monitoring, logging, and scheduling Amazon SNS: Notification service for alerts AWS Systems Manager: Parameter Store for secure configuration Development Tools Python 3.11: Primary programming language Boto3: AWS SDK for Python Requests: HTTP library for API calls JSON: Data format for weather information External Services OpenWeatherMap API: Weather data provider REST APIs: HTTP-based data exchange Best Practices Implemented Security IAM roles with least privilege access Encrypted storage for API keys VPC endpoints for internal AWS communication No hardcoded credentials in code Reliability Comprehensive error handling and retry logic Dead letter queues for failed executions Health checks and monitoring Graceful degradation on partial failures Scalability Serverless architecture with auto-scaling Partitioned data storage for efficient querying Batch processing for multiple cities Rate limiting compliance with API providers Cost Optimization Efficient Lambda memory allocation S3 lifecycle policies for data archival CloudWatch log retention management API usage monitoring and optimization Prerequisites for This Module Before starting, ensure you have:\nAWS account with Lambda, S3, CloudWatch access AWS CLI configured with appropriate permissions OpenWeatherMap account (free registration) Basic Python programming knowledge Understanding of JSON data structures Success Criteria By the end of this module, you should have:\nAutomated weather data collection running every hour Weather forecast collection running every 6 hours Properly structured data stored in S3 Monitoring dashboard showing collection metrics Error alerting system via email notifications API usage tracking staying within free tier limits Ready to start collecting weather data? Let\u0026rsquo;s begin with setting up your OpenWeatherMap API access!\nModule Duration: Approximately 90 minutes\nSection 2.1: 20 minutes (API setup) Section 2.2: 35 minutes (Lambda development) Section 2.3: 25 minutes (Scheduling setup) Section 2.4: 10 minutes (Testing) Important: Keep track of your OpenWeatherMap API key and AWS resource names as you\u0026rsquo;ll need them in subsequent modules.\n"
},
{
	"uri": "//localhost:1313/3-serverless-processing-lambda/",
	"title": "Data Processing and Transformation",
	"tags": [],
	"description": "",
	"content": "Data Processing and Transformation In this module, you\u0026rsquo;ll build a Lambda function to process and transform the raw weather data collected from OpenWeatherMap API into a more analytics-friendly format. This transformation step is essential in any ETL pipeline to prepare data for efficient analysis.\nModule Overview Raw weather data from APIs often contains complex nested structures, inconsistent formats, and extraneous information. In this module, we\u0026rsquo;ll transform this raw data into a clean, structured format optimized for analytics.\nDuration: 45-60 minutes\nCost: ~$0.50\nWhat You\u0026rsquo;ll Build graph LR A[S3 Raw Data] --\u0026gt; B[Lambda Processor] B --\u0026gt; C[S3 Processed Data] D[CloudWatch Events] --\u0026gt; B style B fill:#ff9900,stroke:#232f3e,stroke-width:3px style A fill:#f3e5f5 style C fill:#f3e5f5 Prerequisites Completed Module 2: Weather Data Collection Raw weather data already being collected in S3 AWS CLI configured Step 1: Create the Data Processing Lambda Function 1.1 Create Lambda Function Directory Create a new directory for your Lambda function:\nmkdir weather-processor cd weather-processor 1.2 Create the Complete Lambda Function Create lambda_function.py with the following code:\nimport json import boto3 import datetime import logging from decimal import Decimal import os # Configure logging logger = logging.getLogger() logger.setLevel(logging.INFO) # Initialize AWS clients s3_client = boto3.client(\u0026#39;s3\u0026#39;) def lambda_handler(event, context): \u0026#34;\u0026#34;\u0026#34; Main Lambda handler for processing weather data \u0026#34;\u0026#34;\u0026#34; try: processed_bucket = os.environ[\u0026#39;PROCESSED_BUCKET\u0026#39;] processed_count = 0 # Process each S3 event record for record in event[\u0026#39;Records\u0026#39;]: try: # Extract bucket and key from event source_bucket = record[\u0026#39;s3\u0026#39;][\u0026#39;bucket\u0026#39;][\u0026#39;name\u0026#39;] source_key = record[\u0026#39;s3\u0026#39;][\u0026#39;object\u0026#39;][\u0026#39;key\u0026#39;] logger.info(f\u0026#34;Processing file: {source_key} from bucket: {source_bucket}\u0026#34;) # Get the raw weather data from S3 response = s3_client.get_object(Bucket=source_bucket, Key=source_key) raw_data = json.loads(response[\u0026#39;Body\u0026#39;].read().decode(\u0026#39;utf-8\u0026#39;)) # Transform the weather data processed_data = transform_weather_data(raw_data) # Create processed file key processed_key = source_key.replace(\u0026#39;raw/\u0026#39;, \u0026#39;processed/\u0026#39;).replace(\u0026#39;.json\u0026#39;, \u0026#39;_processed.json\u0026#39;) # Save processed data to S3 s3_client.put_object( Bucket=processed_bucket, Key=processed_key, Body=json.dumps(processed_data, indent=2, default=decimal_default), ContentType=\u0026#39;application/json\u0026#39; ) processed_count += 1 logger.info(f\u0026#34;Successfully processed and saved: {processed_key}\u0026#34;) except Exception as e: logger.error(f\u0026#34;Error processing record {source_key}: {str(e)}\u0026#34;) continue return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: json.dumps({ \u0026#39;message\u0026#39;: f\u0026#39;Successfully processed {processed_count} files\u0026#39;, \u0026#39;processedCount\u0026#39;: processed_count }) } except Exception as e: logger.error(f\u0026#34;Lambda execution error: {str(e)}\u0026#34;) return { \u0026#39;statusCode\u0026#39;: 500, \u0026#39;body\u0026#39;: json.dumps({ \u0026#39;error\u0026#39;: str(e) }) } def transform_weather_data(raw_data): \u0026#34;\u0026#34;\u0026#34; Transform raw OpenWeatherMap data into analytics-friendly format \u0026#34;\u0026#34;\u0026#34; try: # Extract timestamp timestamp = datetime.datetime.fromtimestamp(raw_data[\u0026#39;dt\u0026#39;]).isoformat() + \u0026#39;Z\u0026#39; collection_date = datetime.datetime.fromtimestamp(raw_data[\u0026#39;dt\u0026#39;]).strftime(\u0026#39;%Y-%m-%d\u0026#39;) # Extract basic location and weather info processed_data = { \u0026#39;timestamp\u0026#39;: timestamp, \u0026#39;city_name\u0026#39;: raw_data.get(\u0026#39;name\u0026#39;, \u0026#39;Unknown\u0026#39;), \u0026#39;country\u0026#39;: raw_data.get(\u0026#39;sys\u0026#39;, {}).get(\u0026#39;country\u0026#39;, \u0026#39;Unknown\u0026#39;), \u0026#39;latitude\u0026#39;: raw_data.get(\u0026#39;coord\u0026#39;, {}).get(\u0026#39;lat\u0026#39;), \u0026#39;longitude\u0026#39;: raw_data.get(\u0026#39;coord\u0026#39;, {}).get(\u0026#39;lon\u0026#39;), \u0026#39;data_collection_date\u0026#39;: collection_date } # Temperature conversions temp_kelvin = raw_data.get(\u0026#39;main\u0026#39;, {}).get(\u0026#39;temp\u0026#39;) if temp_kelvin: processed_data[\u0026#39;temperature_kelvin\u0026#39;] = temp_kelvin processed_data[\u0026#39;temperature_celsius\u0026#39;] = round(temp_kelvin - 273.15, 2) processed_data[\u0026#39;temperature_fahrenheit\u0026#39;] = round((temp_kelvin - 273.15) * 9/5 + 32, 2) # Feels like temperature feels_like_kelvin = raw_data.get(\u0026#39;main\u0026#39;, {}).get(\u0026#39;feels_like\u0026#39;) if feels_like_kelvin: processed_data[\u0026#39;feels_like_celsius\u0026#39;] = round(feels_like_kelvin - 273.15, 2) processed_data[\u0026#39;feels_like_fahrenheit\u0026#39;] = round((feels_like_kelvin - 273.15) * 9/5 + 32, 2) # Other weather parameters processed_data.update({ \u0026#39;humidity_percent\u0026#39;: raw_data.get(\u0026#39;main\u0026#39;, {}).get(\u0026#39;humidity\u0026#39;), \u0026#39;pressure_hpa\u0026#39;: raw_data.get(\u0026#39;main\u0026#39;, {}).get(\u0026#39;pressure\u0026#39;), \u0026#39;visibility_meters\u0026#39;: raw_data.get(\u0026#39;visibility\u0026#39;), \u0026#39;uv_index\u0026#39;: raw_data.get(\u0026#39;uvi\u0026#39;) # If available }) # Weather description weather_list = raw_data.get(\u0026#39;weather\u0026#39;, []) if weather_list: weather = weather_list[0] processed_data.update({ \u0026#39;weather_id\u0026#39;: weather.get(\u0026#39;id\u0026#39;), \u0026#39;weather_main\u0026#39;: weather.get(\u0026#39;main\u0026#39;), \u0026#39;weather_description\u0026#39;: weather.get(\u0026#39;description\u0026#39;), \u0026#39;weather_icon\u0026#39;: weather.get(\u0026#39;icon\u0026#39;) }) # Wind information wind_data = raw_data.get(\u0026#39;wind\u0026#39;, {}) processed_data.update({ \u0026#39;wind_speed_ms\u0026#39;: wind_data.get(\u0026#39;speed\u0026#39;), \u0026#39;wind_direction_deg\u0026#39;: wind_data.get(\u0026#39;deg\u0026#39;), \u0026#39;wind_gust_ms\u0026#39;: wind_data.get(\u0026#39;gust\u0026#39;) }) # Convert wind speed to km/h and mph if wind_data.get(\u0026#39;speed\u0026#39;): processed_data[\u0026#39;wind_speed_kmh\u0026#39;] = round(wind_data[\u0026#39;speed\u0026#39;] * 3.6, 2) processed_data[\u0026#39;wind_speed_mph\u0026#39;] = round(wind_data[\u0026#39;speed\u0026#39;] * 2.237, 2) # Cloud coverage processed_data[\u0026#39;cloud_coverage_percent\u0026#39;] = raw_data.get(\u0026#39;clouds\u0026#39;, {}).get(\u0026#39;all\u0026#39;) # Precipitation (if available) rain_data = raw_data.get(\u0026#39;rain\u0026#39;, {}) if rain_data: processed_data[\u0026#39;rain_1h_mm\u0026#39;] = rain_data.get(\u0026#39;1h\u0026#39;) processed_data[\u0026#39;rain_3h_mm\u0026#39;] = rain_data.get(\u0026#39;3h\u0026#39;) snow_data = raw_data.get(\u0026#39;snow\u0026#39;, {}) if snow_data: processed_data[\u0026#39;snow_1h_mm\u0026#39;] = snow_data.get(\u0026#39;1h\u0026#39;) processed_data[\u0026#39;snow_3h_mm\u0026#39;] = snow_data.get(\u0026#39;3h\u0026#39;) # Add derived fields processed_data.update(calculate_derived_fields(processed_data)) return processed_data except Exception as e: logger.error(f\u0026#34;Error transforming weather data: {str(e)}\u0026#34;) raise def calculate_derived_fields(data): \u0026#34;\u0026#34;\u0026#34; Calculate derived weather metrics \u0026#34;\u0026#34;\u0026#34; derived = {} try: # Heat Index calculation (simplified) temp_f = data.get(\u0026#39;temperature_fahrenheit\u0026#39;) humidity = data.get(\u0026#39;humidity_percent\u0026#39;) if temp_f and humidity: if temp_f \u0026gt;= 80: # Heat index only meaningful above 80°F # Simplified heat index formula heat_index_f = ( -42.379 + 2.04901523 * temp_f + 10.14333127 * humidity - 0.22475541 * temp_f * humidity - 6.83783e-3 * temp_f**2 - 5.481717e-2 * humidity**2 + 1.22874e-3 * temp_f**2 * humidity + 8.5282e-4 * temp_f * humidity**2 - 1.99e-6 * temp_f**2 * humidity**2 ) derived[\u0026#39;heat_index_fahrenheit\u0026#39;] = round(heat_index_f, 2) derived[\u0026#39;heat_index_celsius\u0026#39;] = round((heat_index_f - 32) * 5/9, 2) # Comfort level based on temperature and humidity temp_c = data.get(\u0026#39;temperature_celsius\u0026#39;) if temp_c and humidity: if temp_c \u0026lt; 10: comfort = \u0026#39;cold\u0026#39; elif temp_c \u0026lt; 18: comfort = \u0026#39;cool\u0026#39; elif temp_c \u0026lt;= 24 and humidity \u0026lt;= 60: comfort = \u0026#39;comfortable\u0026#39; elif temp_c \u0026lt;= 30 and humidity \u0026lt;= 70: comfort = \u0026#39;warm\u0026#39; else: comfort = \u0026#39;hot\u0026#39; derived[\u0026#39;comfort_level\u0026#39;] = comfort # Wind condition wind_speed_kmh = data.get(\u0026#39;wind_speed_kmh\u0026#39;) if wind_speed_kmh: if wind_speed_kmh \u0026lt; 5: wind_condition = \u0026#39;calm\u0026#39; elif wind_speed_kmh \u0026lt; 20: wind_condition = \u0026#39;light\u0026#39; elif wind_speed_kmh \u0026lt; 40: wind_condition = \u0026#39;moderate\u0026#39; elif wind_speed_kmh \u0026lt; 60: wind_condition = \u0026#39;strong\u0026#39; else: wind_condition = \u0026#39;very_strong\u0026#39; derived[\u0026#39;wind_condition\u0026#39;] = wind_condition # Weather severity weather_main = data.get(\u0026#39;weather_main\u0026#39;, \u0026#39;\u0026#39;).lower() if weather_main: if weather_main in [\u0026#39;thunderstorm\u0026#39;, \u0026#39;tornado\u0026#39;]: severity = \u0026#39;severe\u0026#39; elif weather_main in [\u0026#39;rain\u0026#39;, \u0026#39;snow\u0026#39;, \u0026#39;drizzle\u0026#39;]: severity = \u0026#39;moderate\u0026#39; elif weather_main in [\u0026#39;mist\u0026#39;, \u0026#39;fog\u0026#39;, \u0026#39;haze\u0026#39;]: severity = \u0026#39;mild\u0026#39; else: severity = \u0026#39;normal\u0026#39; derived[\u0026#39;weather_severity\u0026#39;] = severity return derived except Exception as e: logger.error(f\u0026#34;Error calculating derived fields: {str(e)}\u0026#34;) return {} def decimal_default(obj): \u0026#34;\u0026#34;\u0026#34; JSON serializer for objects not serializable by default \u0026#34;\u0026#34;\u0026#34; if isinstance(obj, Decimal): return float(obj) raise TypeError 1.3 Create Requirements File Create requirements.txt:\nboto3==1.34.0 Step 2: Create S3 Bucket for Processed Data # Create bucket for processed data aws s3 mb s3://your-weather-processed-bucket-name # Enable versioning aws s3api put-bucket-versioning \\ --bucket your-weather-processed-bucket-name \\ --versioning-configuration Status=Enabled Step 3: Package and Deploy Lambda Function 3.1 Create Deployment Package # Install dependencies pip install -r requirements.txt -t . # Create deployment package zip -r weather-processor.zip . 3.2 Create IAM Role for Lambda Create trust-policy.json:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;lambda.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34; } ] } Create lambda-policy.json:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:logs:*:*:*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [\u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:PutObject\u0026#34;], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::your-weather-raw-bucket/*\u0026#34;, \u0026#34;arn:aws:s3:::your-weather-processed-bucket/*\u0026#34; ] } ] } Create the IAM role:\n# Create the role aws iam create-role \\ --role-name WeatherProcessorRole \\ --assume-role-policy-document file://trust-policy.json # Attach the policy aws iam put-role-policy \\ --role-name WeatherProcessorRole \\ --policy-name WeatherProcessorPolicy \\ --policy-document file://lambda-policy.json 3.3 Deploy Lambda Function # Create the Lambda function aws lambda create-function \\ --function-name weather-data-processor \\ --runtime python3.9 \\ --role arn:aws:iam::YOUR-ACCOUNT-ID:role/WeatherProcessorRole \\ --handler lambda_function.lambda_handler \\ --zip-file fileb://weather-processor.zip \\ --timeout 60 \\ --memory-size 256 \\ --environment Variables=\u0026#39;{ \u0026#34;PROCESSED_BUCKET\u0026#34;:\u0026#34;your-weather-processed-bucket-name\u0026#34; }\u0026#39; Step 4: Set Up S3 Event Trigger Configure S3 to trigger the Lambda function when new files are uploaded:\n4.1 Add Lambda Permission for S3 aws lambda add-permission \\ --function-name weather-data-processor \\ --principal s3.amazonaws.com \\ --statement-id s3-trigger \\ --action lambda:InvokeFunction \\ --source-arn arn:aws:s3:::your-weather-raw-bucket 4.2 Create S3 Event Notification Create notification-config.json:\n{ \u0026#34;LambdaConfigurations\u0026#34;: [ { \u0026#34;Id\u0026#34;: \u0026#34;weather-processor-trigger\u0026#34;, \u0026#34;LambdaFunctionArn\u0026#34;: \u0026#34;arn:aws:lambda:REGION:ACCOUNT-ID:function:weather-data-processor\u0026#34;, \u0026#34;Events\u0026#34;: [\u0026#34;s3:ObjectCreated:*\u0026#34;], \u0026#34;Filter\u0026#34;: { \u0026#34;Key\u0026#34;: { \u0026#34;FilterRules\u0026#34;: [ { \u0026#34;Name\u0026#34;: \u0026#34;prefix\u0026#34;, \u0026#34;Value\u0026#34;: \u0026#34;raw/\u0026#34; }, { \u0026#34;Name\u0026#34;: \u0026#34;suffix\u0026#34;, \u0026#34;Value\u0026#34;: \u0026#34;.json\u0026#34; } ] } } } ] } Apply the configuration:\naws s3api put-bucket-notification-configuration \\ --bucket your-weather-raw-bucket \\ --notification-configuration file://notification-config.json Step 5: Test the Data Processing 5.1 Manual Test Test with a sample weather data file:\n# Upload a test file to trigger processing aws s3 cp test-weather-data.json s3://your-weather-raw-bucket/raw/test-weather-data.json 5.2 Check CloudWatch Logs # View Lambda logs aws logs describe-log-groups --log-group-name-prefix /aws/lambda/weather-data-processor 5.3 Verify Processed Data # List processed files aws s3 ls s3://your-weather-processed-bucket/processed/ # Download and inspect processed data aws s3 cp s3://your-weather-processed-bucket/processed/test-weather-data_processed.json . cat test-weather-data_processed.json | jq . Data Transformation Examples Raw OpenWeatherMap Data { \u0026#34;coord\u0026#34;: { \u0026#34;lon\u0026#34;: 106.6297, \u0026#34;lat\u0026#34;: 10.8231 }, \u0026#34;weather\u0026#34;: [ { \u0026#34;id\u0026#34;: 803, \u0026#34;main\u0026#34;: \u0026#34;Clouds\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;broken clouds\u0026#34;, \u0026#34;icon\u0026#34;: \u0026#34;04d\u0026#34; } ], \u0026#34;main\u0026#34;: { \u0026#34;temp\u0026#34;: 305.15, \u0026#34;feels_like\u0026#34;: 309.65, \u0026#34;temp_min\u0026#34;: 305.15, \u0026#34;temp_max\u0026#34;: 305.15, \u0026#34;pressure\u0026#34;: 1013, \u0026#34;humidity\u0026#34;: 74 }, \u0026#34;wind\u0026#34;: { \u0026#34;speed\u0026#34;: 3.2, \u0026#34;deg\u0026#34;: 220 }, \u0026#34;clouds\u0026#34;: { \u0026#34;all\u0026#34;: 75 }, \u0026#34;dt\u0026#34;: 1642248000, \u0026#34;sys\u0026#34;: { \u0026#34;country\u0026#34;: \u0026#34;VN\u0026#34;, \u0026#34;sunrise\u0026#34;: 1642203600, \u0026#34;sunset\u0026#34;: 1642245600 }, \u0026#34;timezone\u0026#34;: 25200, \u0026#34;id\u0026#34;: 1566083, \u0026#34;name\u0026#34;: \u0026#34;Ho Chi Minh City\u0026#34; } Transformed Data { \u0026#34;timestamp\u0026#34;: \u0026#34;2025-01-15T09:00:00Z\u0026#34;, \u0026#34;city_name\u0026#34;: \u0026#34;Ho Chi Minh City\u0026#34;, \u0026#34;country\u0026#34;: \u0026#34;VN\u0026#34;, \u0026#34;latitude\u0026#34;: 10.8231, \u0026#34;longitude\u0026#34;: 106.6297, \u0026#34;temperature_celsius\u0026#34;: 32.0, \u0026#34;temperature_fahrenheit\u0026#34;: 89.6, \u0026#34;feels_like_celsius\u0026#34;: 36.5, \u0026#34;feels_like_fahrenheit\u0026#34;: 97.7, \u0026#34;humidity_percent\u0026#34;: 74, \u0026#34;pressure_hpa\u0026#34;: 1013, \u0026#34;weather_main\u0026#34;: \u0026#34;Clouds\u0026#34;, \u0026#34;weather_description\u0026#34;: \u0026#34;broken clouds\u0026#34;, \u0026#34;wind_speed_ms\u0026#34;: 3.2, \u0026#34;wind_speed_kmh\u0026#34;: 11.52, \u0026#34;wind_speed_mph\u0026#34;: 7.16, \u0026#34;wind_direction_deg\u0026#34;: 220, \u0026#34;cloud_coverage_percent\u0026#34;: 75, \u0026#34;heat_index_celsius\u0026#34;: 38.2, \u0026#34;heat_index_fahrenheit\u0026#34;: 100.8, \u0026#34;comfort_level\u0026#34;: \u0026#34;hot\u0026#34;, \u0026#34;wind_condition\u0026#34;: \u0026#34;light\u0026#34;, \u0026#34;weather_severity\u0026#34;: \u0026#34;normal\u0026#34;, \u0026#34;data_collection_date\u0026#34;: \u0026#34;2025-01-15\u0026#34; } Monitoring and Troubleshooting CloudWatch Metrics Monitor these key metrics:\nLambda invocations Errors and duration S3 bucket metrics Common Issues Permission Errors: Ensure Lambda has proper S3 permissions Memory Issues: Increase Lambda memory if processing large files Timeout: Adjust Lambda timeout for complex transformations Benefits of Data Transformation Improved Query Performance: Flat structures are easier to query Reduced Storage Costs: Optimized data formats use less storage Enhanced Analytics: Derived fields enable deeper insights Better Data Quality: Validation ensures reliable data Next Steps After completing this module, you\u0026rsquo;ll have a fully functional data transformation pipeline that prepares your weather data for analysis. In the next module, we\u0026rsquo;ll use Amazon Athena to query and analyze this processed data.\nData transformation is where you can add your domain-specific knowledge. Consider what additional weather metrics might be useful for your specific analysis needs.\nRemember to replace placeholder values (YOUR-ACCOUNT-ID, REGION, bucket names) with your actual AWS account details.\n"
},
{
	"uri": "//localhost:1313/4-data-storage-solutions/",
	"title": "Data Analysis with Athena",
	"tags": [],
	"description": "",
	"content": "Data Analysis with Athena In this module, you\u0026rsquo;ll learn how to use Amazon Athena to run SQL queries directly against your processed weather data stored in S3. Athena is a serverless query service that makes it easy to analyze data using standard SQL without the need to set up complex data warehousing infrastructure.\nModule Overview Amazon Athena allows you to analyze data in S3 using standard SQL queries without having to move the data or set up servers. This makes it an ideal tool for ad-hoc analysis of your weather data. In this module, we\u0026rsquo;ll set up Athena to query our processed weather data and extract meaningful insights.\nDuration: 40-50 minutes\nCost: ~$0.50\nWhat You\u0026rsquo;ll Build graph LR\rA[S3 Processed Data] --\u0026gt; B[Athena Query Service]\rB --\u0026gt; C[SQL Analysis]\rC --\u0026gt; D[Weather Insights]\rstyle B fill:#4fc3f7,stroke:#232f3e,stroke-width:3px\rstyle A fill:#f3e5f5\rstyle D fill:#66bb6a Prerequisites Completed Module 3: Data Processing and Transformation Processed weather data available in S3 AWS Console access Step 1: Set Up Athena Query Result Location Before using Athena, you need to configure a location to store query results.\n1.1 Create S3 Bucket for Query Results # Create bucket for Athena query results aws s3 mb s3://your-athena-query-results-bucket # Optional: Set lifecycle policy to delete old results after 30 days cat \u0026gt; lifecycle-policy.json \u0026lt;\u0026lt; EOF { \u0026#34;Rules\u0026#34;: [ { \u0026#34;ID\u0026#34;: \u0026#34;DeleteOldQueryResults\u0026#34;, \u0026#34;Status\u0026#34;: \u0026#34;Enabled\u0026#34;, \u0026#34;Filter\u0026#34;: {\u0026#34;Prefix\u0026#34;: \u0026#34;\u0026#34;}, \u0026#34;Expiration\u0026#34;: {\u0026#34;Days\u0026#34;: 30} } ] } EOF aws s3api put-bucket-lifecycle-configuration \\ --bucket your-athena-query-results-bucket \\ --lifecycle-configuration file://lifecycle-policy.json 1.2 Configure Athena Query Result Location Open the Amazon Athena console Click Settings in the top right Click Manage Enter your S3 bucket path: s3://your-athena-query-results-bucket/ Click Save Step 2: Create Database and Table 2.1 Create Weather Analytics Database In the Athena Query Editor, run:\nCREATE DATABASE IF NOT EXISTS weather_analytics COMMENT \u0026#39;Database for weather data analysis\u0026#39; LOCATION \u0026#39;s3://your-athena-query-results-bucket/databases/weather_analytics/\u0026#39;; 2.2 Create External Table for Processed Weather Data CREATE EXTERNAL TABLE IF NOT EXISTS weather_analytics.current_weather ( timestamp STRING, city_name STRING, country STRING, latitude DOUBLE, longitude DOUBLE, data_collection_date STRING, temperature_kelvin DOUBLE, temperature_celsius DOUBLE, temperature_fahrenheit DOUBLE, feels_like_celsius DOUBLE, feels_like_fahrenheit DOUBLE, humidity_percent INT, pressure_hpa INT, visibility_meters BIGINT, uv_index DOUBLE, weather_id INT, weather_main STRING, weather_description STRING, weather_icon STRING, wind_speed_ms DOUBLE, wind_speed_kmh DOUBLE, wind_speed_mph DOUBLE, wind_direction_deg INT, wind_gust_ms DOUBLE, cloud_coverage_percent INT, rain_1h_mm DOUBLE, rain_3h_mm DOUBLE, snow_1h_mm DOUBLE, snow_3h_mm DOUBLE, heat_index_fahrenheit DOUBLE, heat_index_celsius DOUBLE, comfort_level STRING, wind_condition STRING, weather_severity STRING ) ROW FORMAT SERDE \u0026#39;org.openx.data.jsonserde.JsonSerDe\u0026#39; WITH SERDEPROPERTIES ( \u0026#39;serialization.format\u0026#39; = \u0026#39;1\u0026#39; ) LOCATION \u0026#39;s3://your-weather-processed-bucket/processed/\u0026#39; TBLPROPERTIES ( \u0026#39;has_encrypted_data\u0026#39;=\u0026#39;false\u0026#39;, \u0026#39;projection.enabled\u0026#39;=\u0026#39;true\u0026#39;, \u0026#39;projection.data_collection_date.type\u0026#39;=\u0026#39;date\u0026#39;, \u0026#39;projection.data_collection_date.range\u0026#39;=\u0026#39;2025-01-01,NOW\u0026#39;, \u0026#39;projection.data_collection_date.format\u0026#39;=\u0026#39;yyyy-MM-dd\u0026#39;, \u0026#39;storage.location.template\u0026#39;=\u0026#39;s3://your-weather-processed-bucket/processed/year=${data_collection_date}\u0026#39; ); 2.3 Verify Table Creation -- Check if table was created successfully SHOW TABLES IN weather_analytics; -- Get table schema DESCRIBE weather_analytics.current_weather; -- Count total records SELECT COUNT(*) as total_records FROM weather_analytics.current_weather; Step 3: Basic Weather Data Analysis 3.1 Data Exploration Queries View Sample Data -- Get a sample of your data SELECT * FROM weather_analytics.current_weather LIMIT 10; Check Data Quality -- Check for missing values in key fields SELECT COUNT(*) as total_records, COUNT(temperature_celsius) as temp_records, COUNT(humidity_percent) as humidity_records, COUNT(pressure_hpa) as pressure_records, COUNT(wind_speed_ms) as wind_records FROM weather_analytics.current_weather; Date Range Analysis -- Check the date range of your data SELECT MIN(data_collection_date) as earliest_date, MAX(data_collection_date) as latest_date, COUNT(DISTINCT data_collection_date) as unique_dates FROM weather_analytics.current_weather; 3.2 Temperature Analysis Average Temperature by City SELECT city_name, country, COUNT(*) as measurement_count, ROUND(AVG(temperature_celsius), 2) as avg_temp_celsius, ROUND(MIN(temperature_celsius), 2) as min_temp_celsius, ROUND(MAX(temperature_celsius), 2) as max_temp_celsius FROM weather_analytics.current_weather GROUP BY city_name, country ORDER BY avg_temp_celsius DESC; Temperature Trends Over Time SELECT data_collection_date, city_name, ROUND(AVG(temperature_celsius), 2) as avg_temp, ROUND(AVG(feels_like_celsius), 2) as avg_feels_like FROM weather_analytics.current_weather WHERE data_collection_date \u0026gt;= DATE(\u0026#39;2025-01-01\u0026#39;) GROUP BY data_collection_date, city_name ORDER BY data_collection_date, city_name; Heat Index Analysis SELECT city_name, ROUND(AVG(heat_index_celsius), 2) as avg_heat_index, comfort_level, COUNT(*) as occurrence_count FROM weather_analytics.current_weather WHERE heat_index_celsius IS NOT NULL GROUP BY city_name, comfort_level ORDER BY avg_heat_index DESC; 3.3 Weather Pattern Analysis Weather Condition Distribution SELECT weather_main, weather_description, COUNT(*) as occurrence_count, ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER (), 2) as percentage FROM weather_analytics.current_weather GROUP BY weather_main, weather_description ORDER BY occurrence_count DESC; Wind Analysis SELECT city_name, wind_condition, COUNT(*) as occurrence_count, ROUND(AVG(wind_speed_kmh), 2) as avg_wind_speed_kmh, ROUND(MAX(wind_speed_kmh), 2) as max_wind_speed_kmh FROM weather_analytics.current_weather GROUP BY city_name, wind_condition ORDER BY city_name, avg_wind_speed_kmh DESC; Humidity vs Temperature Correlation SELECT city_name, CASE WHEN temperature_celsius \u0026lt; 15 THEN \u0026#39;Cold\u0026#39; WHEN temperature_celsius \u0026lt; 25 THEN \u0026#39;Moderate\u0026#39; ELSE \u0026#39;Hot\u0026#39; END as temp_category, ROUND(AVG(humidity_percent), 2) as avg_humidity, ROUND(AVG(temperature_celsius), 2) as avg_temperature, COUNT(*) as sample_count FROM weather_analytics.current_weather GROUP BY city_name, CASE WHEN temperature_celsius \u0026lt; 15 THEN \u0026#39;Cold\u0026#39; WHEN temperature_celsius \u0026lt; 25 THEN \u0026#39;Moderate\u0026#39; ELSE \u0026#39;Hot\u0026#39; END ORDER BY city_name, temp_category; Step 4: Advanced Analytics Queries 4.1 Time-Series Analysis Daily Weather Summary SELECT data_collection_date, COUNT(DISTINCT city_name) as cities_measured, ROUND(AVG(temperature_celsius), 2) as global_avg_temp, ROUND(AVG(humidity_percent), 2) as global_avg_humidity, ROUND(AVG(pressure_hpa), 2) as global_avg_pressure FROM weather_analytics.current_weather GROUP BY data_collection_date ORDER BY data_collection_date; Weather Extremes -- Find temperature extremes WITH temp_extremes AS ( SELECT data_collection_date, city_name, temperature_celsius, ROW_NUMBER() OVER (PARTITION BY data_collection_date ORDER BY temperature_celsius DESC) as hot_rank, ROW_NUMBER() OVER (PARTITION BY data_collection_date ORDER BY temperature_celsius ASC) as cold_rank FROM weather_analytics.current_weather ) SELECT data_collection_date, MAX(CASE WHEN hot_rank = 1 THEN city_name END) as hottest_city, MAX(CASE WHEN hot_rank = 1 THEN temperature_celsius END) as highest_temp, MAX(CASE WHEN cold_rank = 1 THEN city_name END) as coldest_city, MAX(CASE WHEN cold_rank = 1 THEN temperature_celsius END) as lowest_temp FROM temp_extremes WHERE hot_rank = 1 OR cold_rank = 1 GROUP BY data_collection_date ORDER BY data_collection_date; 4.2 Geographical Analysis Weather by Geographic Region SELECT CASE WHEN latitude \u0026gt; 23.5 THEN \u0026#39;Northern Temperate\u0026#39; WHEN latitude \u0026gt; 0 THEN \u0026#39;Northern Tropical\u0026#39; WHEN latitude \u0026gt; -23.5 THEN \u0026#39;Southern Tropical\u0026#39; ELSE \u0026#39;Southern Temperate\u0026#39; END as climate_zone, COUNT(*) as measurement_count, ROUND(AVG(temperature_celsius), 2) as avg_temperature, ROUND(AVG(humidity_percent), 2) as avg_humidity, ROUND(AVG(wind_speed_kmh), 2) as avg_wind_speed FROM weather_analytics.current_weather GROUP BY CASE WHEN latitude \u0026gt; 23.5 THEN \u0026#39;Northern Temperate\u0026#39; WHEN latitude \u0026gt; 0 THEN \u0026#39;Northern Tropical\u0026#39; WHEN latitude \u0026gt; -23.5 THEN \u0026#39;Southern Tropical\u0026#39; ELSE \u0026#39;Southern Temperate\u0026#39; END ORDER BY avg_temperature DESC; Step 5: Export and Visualize Results 5.1 Save Query Results Run any query in Athena Click Download to save results as CSV Results are also automatically saved to your S3 query results bucket 5.2 Create Simple Visualizations You can use the exported CSV files to create charts in:\nExcel or Google Sheets for basic charts Python/Jupyter for advanced analysis QuickSight (covered in next module) 5.3 Example Analysis Workflow -- Create a comprehensive weather report SELECT city_name, country, COUNT(*) as total_measurements, ROUND(AVG(temperature_celsius), 1) as avg_temp_c, ROUND(MIN(temperature_celsius), 1) as min_temp_c, ROUND(MAX(temperature_celsius), 1) as max_temp_c, ROUND(AVG(humidity_percent), 1) as avg_humidity, ROUND(AVG(pressure_hpa), 1) as avg_pressure, ROUND(AVG(wind_speed_kmh), 1) as avg_wind_kmh, MODE() WITHIN GROUP (ORDER BY weather_main) as most_common_weather, MODE() WITHIN GROUP (ORDER BY comfort_level) as most_common_comfort FROM weather_analytics.current_weather GROUP BY city_name, country HAVING COUNT(*) \u0026gt;= 5 -- Only cities with at least 5 measurements ORDER BY avg_temp_c DESC; Step 6: Performance Optimization 6.1 Query Performance Tips Use LIMIT for exploratory queries: SELECT * FROM weather_analytics.current_weather WHERE data_collection_date = \u0026#39;2025-01-15\u0026#39; LIMIT 100; Filter on partition columns: -- Good - filters on partitioned column SELECT * FROM weather_analytics.current_weather WHERE data_collection_date BETWEEN \u0026#39;2025-01-01\u0026#39; AND \u0026#39;2025-01-07\u0026#39;; Select only needed columns: -- Better performance SELECT city_name, temperature_celsius, humidity_percent FROM weather_analytics.current_weather; 6.2 Cost Optimization Monitor your Athena usage:\nCheck CloudWatch metrics for data scanned Use Athena Query History to analyze costs Consider converting to Parquet format for large datasets Troubleshooting Common Issues Issue 1: \u0026ldquo;Table not found\u0026rdquo; Error Verify your S3 bucket permissions Check that data exists in the specified S3 location Ensure proper table schema matches your data Issue 2: \u0026ldquo;Zero records returned\u0026rdquo; Verify data format matches table schema Check S3 path in table definition Ensure files are in JSON format as expected Issue 3: \u0026ldquo;Access denied\u0026rdquo; Error Check IAM permissions for Athena and S3 Verify query results bucket permissions Cost Analysis Typical costs for this module:\nAthena queries: ~$5 per TB of data scanned S3 storage: ~$0.023 per GB/month S3 requests: ~$0.0004 per 1,000 requests For a typical weather dataset (1-10 MB), expect less than $0.50 total cost.\nNext Steps After completing this module, you\u0026rsquo;ll be able to analyze your weather data using SQL queries in Athena. In the next module, we\u0026rsquo;ll build on this foundation to create interactive dashboards using Amazon QuickSight.\nWhen writing Athena queries, always try to limit the amount of data scanned by using appropriate WHERE clauses on date fields and other frequently filtered columns.\nSave your useful queries as Saved Queries in Athena for reuse. You can also create Views for commonly used complex queries.\nRemember to replace placeholder bucket names with your actual S3 bucket names in all SQL queries.\n"
},
{
	"uri": "//localhost:1313/5-analytics-visualization/",
	"title": "Data Visualization with QuickSight",
	"tags": [],
	"description": "",
	"content": "Data Visualization with Amazon QuickSight Now that you have weather data stored and queryable through Athena, it\u0026rsquo;s time to create compelling visualizations! In this module, you\u0026rsquo;ll use Amazon QuickSight to build interactive dashboards that bring your weather data to life.\nModule Overview Amazon QuickSight is AWS\u0026rsquo;s business intelligence (BI) service that makes it easy to create and publish interactive dashboards. You\u0026rsquo;ll connect QuickSight to your Athena data source and create visualizations that reveal weather patterns and insights.\nDuration: 45-60 minutes\nCost: ~$3-4 (using free trial)\nWhat You\u0026rsquo;ll Build graph LR\rA[Athena Data] --\u0026gt; B[QuickSight Dataset]\rB --\u0026gt; C[Visualizations]\rC --\u0026gt; D[Interactive Dashboard]\rD --\u0026gt; E[Published Reports]\rstyle B fill:#ff9900,stroke:#232f3e,stroke-width:3px\rstyle D fill:#66bb6a Prerequisites Completed Module 4: Data Analysis with Athena Weather data available in Athena tables AWS account with QuickSight permissions Step 1: Set Up Amazon QuickSight 1.1 Sign Up for QuickSight Navigate to the Amazon QuickSight console Click Sign up for QuickSight Choose Standard Edition (includes 30-day free trial) Enter your account details: Account name: weather-analytics-[your-name] Notification email: Your email address Click Finish 1.2 Configure QuickSight Permissions In QuickSight, click your profile icon (top right) Select Manage QuickSight Choose Security \u0026amp; permissions Click Add or remove Enable the following services: ✅ Amazon Athena ✅ Amazon S3 For S3, click Select S3 buckets Choose your weather data buckets: your-weather-processed-bucket your-athena-query-results-bucket Click Update Step 2: Create Data Source and Dataset 2.1 Connect to Athena In QuickSight homepage, click Datasets Click New dataset Choose Athena as data source Configure the connection: Data source name: Weather-Data-Athena Athena workgroup: primary (default) Click Create data source 2.2 Create Dataset from Weather Table Select your database: weather_analytics Choose table: current_weather Select Directly query your data Click Visualize If your dataset is small (\u0026lt; 1GB), you can choose \u0026ldquo;Import to SPICE\u0026rdquo; for faster performance. SPICE is QuickSight\u0026rsquo;s in-memory calculation engine.\nStep 3: Create Weather Visualizations 3.1 Temperature Trend Line Chart Create new analysis:\nClick + Add → Add visual Select Line chart Configure the chart:\nX-axis: Drag data_collection_date to X-axis Value: Drag temperature_celsius to Value Color: Drag city_name to Color Customize the chart:\nClick the visual → Format visual Title: \u0026ldquo;Temperature Trends by City\u0026rdquo; Y-axis label: \u0026ldquo;Temperature (°C)\u0026rdquo; Legend: Position at bottom 3.2 Weather Condition Pie Chart Add new visual:\nClick + Add → Add visual Select Pie chart Configure the chart:\nGroup/Color: Drag weather_main to Group/Color Value: Drag city_name to Value Aggregate: Change to Count Customize:\nTitle: \u0026ldquo;Weather Condition Distribution\u0026rdquo; Legend: Show percentages 3.3 City Temperature Comparison Bar Chart Add new visual:\nSelect Vertical bar chart Configure:\nX-axis: Drag city_name to X-axis Value: Drag temperature_celsius to Value Aggregate: Change to Average Customize:\nTitle: \u0026ldquo;Average Temperature by City\u0026rdquo; Y-axis label: \u0026ldquo;Average Temperature (°C)\u0026rdquo; Sort by value (descending) 3.4 Humidity vs Temperature Scatter Plot Add new visual:\nSelect Scatter plot Configure:\nX-axis: Drag temperature_celsius to X-axis Y-axis: Drag humidity_percent to Y-axis Color: Drag comfort_level to Color Size: Drag pressure_hpa to Size Customize:\nTitle: \u0026ldquo;Humidity vs Temperature Correlation\u0026rdquo; X-axis label: \u0026ldquo;Temperature (°C)\u0026rdquo; Y-axis label: \u0026ldquo;Humidity (%)\u0026rdquo; 3.5 Key Performance Indicators (KPIs) Create KPI tiles for latest weather data:\nCurrent Temperature KPI Add new visual → KPI Configure: Value: temperature_celsius Aggregate: Average Filter: Add filter for latest date Title: \u0026ldquo;Current Average Temperature\u0026rdquo; Wind Speed KPI Add KPI visual Configure: Value: wind_speed_kmh Aggregate: Average Title: \u0026ldquo;Current Average Wind Speed\u0026rdquo; Humidity KPI Add KPI visual Configure: Value: humidity_percent Aggregate: Average Title: \u0026ldquo;Current Average Humidity\u0026rdquo; Step 4: Build Comprehensive Dashboard 4.1 Organize Dashboard Layout Resize and arrange visuals:\nPlace KPIs at the top in a row Temperature trend chart in the main area Pie chart and bar chart side by side below Scatter plot at the bottom Add dashboard title:\nClick + Add → Add title Text: \u0026ldquo;Weather Analytics Dashboard\u0026rdquo; Style: Large, center-aligned 4.2 Add Interactive Filters Add date filter:\nClick Filter pane (left side) Click Create one → Choose data_collection_date Filter type: Date range Default: Last 7 days Add city filter:\nCreate filter for city_name Filter type: Multi-select dropdown Show all cities by default Add weather condition filter:\nCreate filter for weather_main Filter type: Multi-select dropdown 4.3 Apply Dashboard Styling Choose color theme:\nClick Themes (top menu) Select Midnight or Classic Customize colors:\nFor temperature chart: Use blue-to-red gradient For weather conditions: Use distinct colors for each condition Add descriptive text:\nClick + Add → Add text box Add insights or instructions for dashboard users Step 5: Advanced Visualizations 5.1 Create Heat Map for Temperature by Time Add new visual → Heat map Configure: Rows: city_name Columns: data_collection_date Values: temperature_celsius (Average) Customize: Title: \u0026ldquo;Temperature Heat Map by City and Date\u0026rdquo; Color scale: Blue (cold) to Red (hot) 5.2 Wind Rose Chart (Using Calculated Fields) Create calculated field:\nClick + Add → Add calculated field Name: wind_direction_category Formula: ifelse(\rwind_direction_deg \u0026gt;= 337.5 OR wind_direction_deg \u0026lt; 22.5, \u0026#34;N\u0026#34;,\rwind_direction_deg \u0026gt;= 22.5 AND wind_direction_deg \u0026lt; 67.5, \u0026#34;NE\u0026#34;,\rwind_direction_deg \u0026gt;= 67.5 AND wind_direction_deg \u0026lt; 112.5, \u0026#34;E\u0026#34;,\rwind_direction_deg \u0026gt;= 112.5 AND wind_direction_deg \u0026lt; 157.5, \u0026#34;SE\u0026#34;,\rwind_direction_deg \u0026gt;= 157.5 AND wind_direction_deg \u0026lt; 202.5, \u0026#34;S\u0026#34;,\rwind_direction_deg \u0026gt;= 202.5 AND wind_direction_deg \u0026lt; 247.5, \u0026#34;SW\u0026#34;,\rwind_direction_deg \u0026gt;= 247.5 AND wind_direction_deg \u0026lt; 292.5, \u0026#34;W\u0026#34;,\r\u0026#34;NW\u0026#34;\r) Create wind direction chart:\nVisual type: Donut chart Group: wind_direction_category Value: Count of records Title: \u0026ldquo;Wind Direction Distribution\u0026rdquo; 5.3 Weather Severity Gauge Create calculated field:\nName: severity_score Formula: ifelse(\rweather_severity = \u0026#34;severe\u0026#34;, 4,\rweather_severity = \u0026#34;moderate\u0026#34;, 3,\rweather_severity = \u0026#34;mild\u0026#34;, 2,\r1\r) Add gauge visual:\nValue: severity_score (Average) Title: \u0026ldquo;Weather Severity Index\u0026rdquo; Range: 1-4 Step 6: Publish and Share Dashboard 6.1 Publish Dashboard Click Share (top right) Click Publish dashboard Dashboard name: \u0026ldquo;Weather Analytics Dashboard\u0026rdquo; Description: \u0026ldquo;Interactive weather data visualization showing temperature trends, weather patterns, and city comparisons\u0026rdquo; Click Publish dashboard 6.2 Set Up Sharing Permissions In the published dashboard, click Share Add users or groups: Enter email addresses of users to share with Set permissions: Viewer or Co-owner Click Share 6.3 Create Public Dashboard (Optional) Click Share → Manage dashboard access Enable Make dashboard public Copy the public URL to share externally Only make dashboards public if your weather data doesn\u0026rsquo;t contain sensitive information.\nStep 7: Set Up Automatic Data Refresh 7.1 Configure Dataset Refresh Go to Datasets Select your weather dataset Click Refresh → Schedule refresh Configure schedule: Frequency: Daily Time: Early morning (e.g., 6 AM) Time zone: Your local timezone Save schedule 7.2 Monitor Refresh Status Check refresh history in Dataset settings Set up notifications for failed refreshes Monitor data freshness indicators in dashboard Step 8: Dashboard Optimization and Best Practices 8.1 Performance Optimization Use SPICE for better performance:\nImport data to SPICE if dataset \u0026lt; 10GB Schedule regular SPICE refreshes Optimize queries:\nUse filters to reduce data scanning Create summary tables in Athena for large datasets Limit visual complexity:\nMax 15-20 data points per chart Use drill-down for detailed views 8.2 User Experience Best Practices Clear visual hierarchy:\nMost important metrics at top Related visuals grouped together Consistent color schemes Interactive elements:\nProvide clear filter instructions Use consistent date ranges Enable cross-filtering between visuals Mobile responsiveness:\nTest dashboard on mobile devices Adjust layout for smaller screens Prioritize key metrics for mobile view Troubleshooting Common Issues Issue 1: \u0026ldquo;Insufficient permissions\u0026rdquo; Error Solution: Check QuickSight IAM permissions for Athena and S3 access Verify bucket permissions in QuickSight security settings Issue 2: Data Not Refreshing Solution: Check Athena query permissions Verify data source connection status Review refresh error logs Issue 3: Slow Dashboard Performance Solution: Import data to SPICE Optimize Athena queries with proper filtering Reduce number of visuals per dashboard Cost Analysis QuickSight costs for this module:\nStandard Edition: $9/month per author (30-day free trial) SPICE storage: $0.25/GB/month Enterprise Edition: $18/month per author (advanced features) Total estimated cost: $0-4 during free trial period\nSecurity Best Practices Access Control:\nUse IAM roles for service permissions Implement row-level security if needed Regular audit of user access Data Protection:\nEnable encryption in transit and at rest Use VPC endpoints for private connectivity Monitor data access patterns Next Steps After completing this module, you\u0026rsquo;ll have:\n✅ Interactive weather dashboard with multiple visualizations ✅ Automated data refresh from your ETL pipeline ✅ Shareable insights for weather analysis ✅ Understanding of QuickSight best practices Coming up in Module 6: Learn how to properly clean up AWS resources and explore advanced enhancements for your weather analytics platform.\nSave your dashboard as a template to quickly create similar dashboards for different datasets or time periods.\nQuickSight offers ML-powered insights. Try the ML Insights feature to automatically discover anomalies and trends in your weather data.\nRemember to cancel your QuickSight subscription after the workshop if you don\u0026rsquo;t plan to continue using it to avoid ongoing charges.\n"
},
{
	"uri": "//localhost:1313/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "//localhost:1313/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]